[
  {
    "objectID": "ScarlettAnalysis.html",
    "href": "ScarlettAnalysis.html",
    "title": "The Interplay of IER and CMV in Data",
    "section": "",
    "text": "List of all the libraries to use for the analysis\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(foreign)\nlibrary(careless)\nlibrary(haven)\nlibrary(readxl)\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCall the dataset\n\n\nCode\n# We'll create a tibble with each variable and on what page it appears\npage_table &lt;- tribble(\n  ~question, ~page,\n   \"ssr1\",   1,\n   \"ssr2\",   1,\n   \"ssr3\",   1,\n   \"ri1\",    1,\n   \"ri2\",    1,\n   \"ri3\",    1,\n   \"wom1\",   1,\n   \"wom2\",   1,\n   \"wom3\",   1,\n   \"cmv1\",   1,\n   \"sat1\",   2,\n   \"sat2\",   2,\n   \"sat3\",   2,\n   \"dj1\",    2,\n   \"dj2\",    2,\n   \"dj3\",    2,\n   \"dj4\",    2,\n   \"ij1\",    3,\n   \"ij2\",    3,\n   \"ij3\",    3,\n   \"ij4\",    3,\n   \"pj1\",    3,\n   \"pj2\",    3,\n   \"pj3\",    3,\n   \"pj4\",    3\n)\n\n# This function will return num_rows scarletts with the structure of example_data and the page structure abov\nscarlett &lt;- function(num_rows, page_table) {\n  1:num_rows |&gt; \n    map(\\(x) {\n      page_table |&gt; group_by(page) |&gt; \n        group_modify(~{\n          # give every question on the page the same value\n          g &lt;- .x |&gt; mutate(value = sample(1:7, 1))\n          # create a vector of normally distributed adjustments for the all questions \n          # with the first being zero\n          adjustments &lt;- c(0, rnorm(nrow(g) - 1, 0, 1) |&gt; round())\n          # adjust each value by the random amount\n          g |&gt; mutate(value = value + adjustments) |&gt;\n            # make sure the values are between 1 and 7\n            mutate(value = pmax(1, pmin(7, value)))\n        }) |&gt; ungroup() |&gt; select(-page) |&gt;\n        # put into tabular format\n        pivot_wider(names_from = question, values_from = value) \n    }) |&gt;\n    list_rbind()\n}\n\nexample_data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\")) |&gt; \n  add_column(cmv1 = 7, .after = \"wom3\") # add the CMV variable\n\n\nRows: 500 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (24): ssr1, ssr2, ssr3, ri1, ri2, ri3, wom1, wom2, wom3, sat1, sat2, sat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nIER_Sim_Data &lt;- scarlett(nrow(example_data) * 0.2, page_table) |&gt; #generate scarletts\n  bind_rows(slice_sample(example_data, n = nrow(example_data) * 0.8)) %&gt;%  # combine with 80% of the original data\n  slice_sample(n = nrow(.)) # shuffle the data\n\nIER_Sim_Data\n\n\n\n  \n\n\n\nCalculate the even-odd consistency score for the personality variables.\n\n\nCode\nIER_Scores&lt;- data.frame(matrix(ncol = 0, nrow = 500))\n\nIER_Scores$EO_score &lt;- evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4))\n\n\nWarning in evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4)): Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n\n\nWarning: The number of items specified by 'factors' does not match the number of columns in 'x'. \n Please check if this is what you want.\n\n\nNow calculate the psychometric synonym scores for ALL of the personality/behavior variables.\nFirst, I create a dataset for the variables. Then, I determine how many item pairs have a correlation greater than 0.60.\n\n\nCode\npsychsyn_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychsyn_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychsyn_cor$cor &gt; .60, na.rm = TRUE)\n\n\n[1] 68\n\n\nIf there are enough pairs, proceed with calculating individual scores. (there were 49)\n\n\nCode\nIER_Scores$psychsyn_score &lt;- psychsyn(IER_Sim_Data, critval = .60)\n\n\nSince positive correlations indicate participants are responding consistently (i.e. carefully), multiply these scores by -1 so that higher scores reflect IER.\n\n\nCode\nIER_Scores$psychsyn_Index &lt;- -1 *(IER_Scores$psychsyn_score)\n\n\nNow calculate the psychometric antonym scores for ALL of the personality/behavior variables.\nFirst, I determine how many item pairs have a correlation less than -0.60.\n\n\nCode\npsychant_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychant_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychant_cor$cor &lt; -.60, na.rm = TRUE)\n\n\n[1] 0\n\n\nIf there are enough pairs, proceed with calculating individual scores. There were 0 in this dataset.\nNext, calculate the IRV index for the personality variables.\nThe intra-individual response variability (IRV) is similiar in spirit to the Longstring index. It is defined as the “standard deviation of responses across a set of consecutive item responses for an individual” (Dunn et al. 2018).\nSince all the items are positively worded, I am calculating this over the whole survey and then splitting the survey into four sections.\n\n\nCode\nIER_Scores$irv_scores &lt;- irv(IER_Sim_Data, split = FALSE)\n\n\nCalculate the long string index for the personality variables.For each observation, the length of the maximum uninterrupted string of identical responses is returned.\n\n\nCode\nIER_Scores$careless_long&lt;-longstring(IER_Sim_Data,avg=FALSE)\n\n\nNow I have to clean up all the NA values in the pyschometric synonym and psychometric antonym indices. NA values index indicate that there was no variance within any of the pairs. Since no variance indicates extreme consistency, and these indices are meant to catch inconsistent respondents. I assign a value of -1 to these respondents because they were responding “carefully,” which really means consistently with these indices.\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$psychsyn_Index)\n\nIER_Scores$psychsyn_Index[missing_rows] &lt;- -1\n\n\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$EO_score)\n\nIER_Scores$EO_score[missing_rows] &lt;- -1\n\n\nCompute Mahalanobis D Index\n\n\nCode\nIER_Scores$mahalscore &lt;- mahad(IER_Sim_Data, plot = FALSE, flag = FALSE, confidence = 0.95, na.rm = TRUE)\n\n\nThe below code converts the IER scores to z-scores.\n\n\nCode\nIER_Sim_Data$ZLongstring_Index &lt;- (IER_Scores$careless_long-mean(IER_Scores$careless_long))/sd(IER_Scores$careless_long)\n\nIER_Sim_Data$ZIRVTotal_Index &lt;- (IER_Scores$irv_scores-mean(IER_Scores$irv_scores))/sd(IER_Scores$irv_scores)\n\nIER_Sim_Data$ZEO_score &lt;- (IER_Scores$EO_score-mean(IER_Scores$EO_score))/sd(IER_Scores$EO_score)\n\nIER_Sim_Data$Zpsychsyn_score &lt;- (IER_Scores$psychsyn_Index-mean(na.omit(IER_Scores$psychsyn_Index)))/sd(na.omit(IER_Scores$psychsyn_Index))\n\nIER_Sim_Data$Zmahal &lt;- (IER_Scores$mahalscore-mean(na.omit(IER_Scores$mahalscore)))/sd(na.omit(IER_Scores$mahalscore))\n\n\nAt this point, we have calculated the scores for all of the IER indices and can move forward the the analyses.",
    "crumbs": [
      "Analyses",
      "Scarlett"
    ]
  },
  {
    "objectID": "ScarlettAnalysis.html#step-one-prepare-the-rstudio-environment",
    "href": "ScarlettAnalysis.html#step-one-prepare-the-rstudio-environment",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step One: Prepare the RStudio Environment",
    "text": "Step One: Prepare the RStudio Environment\nThe below code calls the libraries necessary for the technique.\n\n\nCode\nlibrary(haven)\nlibrary(lavaan)\nlibrary(lavaanPlot)",
    "crumbs": [
      "Analyses",
      "Scarlett"
    ]
  },
  {
    "objectID": "ScarlettAnalysis.html#step-two-model-specification",
    "href": "ScarlettAnalysis.html#step-two-model-specification",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Two: Model Specification",
    "text": "Step Two: Model Specification\nThe lavaan model syntax is used to describe models to be estimated. The syntax defines models using various formula types. The formula types which are used in the technique are below.\nLatent Variable Definition:\nlatent variable =~ indicator1 + indicator2 + indicator3\nVariances and Covariances:\nvariable1 ~~ variable2\nRegression:\nvariable1 ~~ variable2 + variable3 + variable4\nBy default, the lavaan syntax will always fix the factor loading of a latent variable's first indicator to 1. Add the argument std.lv = TRUE to the function call to force this factor loading to be free.\n\nBaseline Model\nFirst, you must establish a baseline model that will be used in subsequent models and test. The baseline model is a confirmatory factor analysis (CFA) model of the substantive variables and IER variables. Estimates of the unstandardized factor loadings and error variances for the IER variables are retained from this model to use as fixed parameters in subsequent models (Williams, 2016). Additionally, the factor correlations between the substantive variables are recorded to be used in a later model.\nThe code to specify the baseline for this example is as follows:\n\n\nCode\nBaselineModel &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal'\n\nBaselineModel_fit &lt;- cfa(BaselineModel, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(BaselineModel_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n493.469 257.000   0.986   0.043 \n\n\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 1\nThe first model to examine is a structural model without IER effects which allows for correlations among the substantive latent variables with an orthogonal IER variable. The IER variable’s indicators are the standardized IER indices retained in the exploratory factor analysis.\nThe IER variable is assumed to be orthogonal, as IER behavior should not be correlated with the substantive variables.\nThe code to specify the Model 1 for this example is as follows:\n\n\nCode\nModel1 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel1_fit &lt;- cfa(Model1, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model1_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n915.123 278.000   0.963   0.068 \n\n\nRecord the substantive factor correlations estimated in this model to use in subsequent models. This can be done by sifting through the lavaan summary output or by utilizing code to extract the estimates from the summary.\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 2\nModel 2 is a saturated structural model with IER effects. With this model, the impact of the IER latent variable is allowed to be different for each of the indicators of the substantive constructs. This is done so that the subsequent test of the presence of IER can be conducted without making any assumptions about the specific nature of these effects.\n\n\nCode\nModel2 &lt;-'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel2_fit &lt;- cfa(Model2, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model2_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n404.463 209.000   0.989   0.043 \n\n\n\n\nModel 3\nModel 3 is identical to Model 2, except that the substantive factor correlations are constrained to their unstandardized estimates from Model 1.\nModel 3 is a restricted saturated structural model with IER effects; the term restricted indicates that the values of the substantively important parameters have been restricted to be equal to the estimates from Model 1. The examination of Model 3 provides a direct statistical test of the impact of IER effects.\nThe code to specify Model 3 for this example is as follows:\n\n\nCode\nModel3 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        \n        fssr ~~ 0.382*fri + 0.936*fwom + 0.311*fsat + 0.805*fdj + 0.602*fij + 0.461*fpj\n        fri ~~ 0.319*fwom + 0.782*fsat + 0.506*fdj + 0.493*fij + 0.547*fpj\n        fwom ~~ 0.260*fsat + 0.692*fdj + 0.655*fij + 0.416*fpj\n        fsat ~~ 0.478*fdj + 0.575*fij + 0.689*fpj\n        fdj ~~ 0.639*fij + 0.586*fpj\n        fij ~~ 0.597*fpj'\n\nModel3_fit &lt;- cfa(Model3, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model3_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n   chisq       df      cfi    rmsea \n1680.483  230.000    0.915    0.112",
    "crumbs": [
      "Analyses",
      "Scarlett"
    ]
  },
  {
    "objectID": "ScarlettAnalysis.html#step-three-compare-the-models",
    "href": "ScarlettAnalysis.html#step-three-compare-the-models",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Three: Compare the models",
    "text": "Step Three: Compare the models\nFirst, a comparison of Model 1 and Model 2 tests the hypothesis that that there are no method effects due to IER.\n\n\nCode\nlavTestLRT(Model1_fit, Model2_fit)\n\n\n\n  \n\n\n\nNext, a comparison of Model 2 and Model 3 tests whether the substantive variable correlations are significantly biased by IER effects.\n\n\nCode\nlavTestLRT(Model3_fit, Model2_fit)\n\n\n\n  \n\n\n\nExport IER Scores\n\n\n Download data",
    "crumbs": [
      "Analyses",
      "Scarlett"
    ]
  },
  {
    "objectID": "DarrenAnalysis.html",
    "href": "DarrenAnalysis.html",
    "title": "Mallory Analysis",
    "section": "",
    "text": "List of all the libraries to use for the analysis\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(foreign)\nlibrary(careless)\nlibrary(haven)\nlibrary(readxl)\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCall the dataset and add 20% Darrens\n\n\nCode\nexample_data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))  |&gt; \n  add_column(cmv1 = 7, .after = \"wom3\") # add the CMV variable\n\n\nRows: 500 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (24): ssr1, ssr2, ssr3, ri1, ri2, ri3, wom1, wom2, wom3, sat1, sat2, sat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndarren &lt;- function(num_rows, example_data) {\n  1:num_rows |&gt; \n    map(\\(x) example_data[1,] |&gt; mutate(across(everything(), ~sample(1:7, 1)))) |&gt; \n    list_rbind()\n}\n# generate data with 20% darrens\nIER_Sim_Data &lt;- darren(nrow(example_data) * 0.2, example_data) |&gt; #generate darrens\n  bind_rows(slice_sample(example_data, n = nrow(example_data) * 0.8)) %&gt;%  # combine with 80% of the original data\n  slice_sample(n = nrow(.)) # shuffle the data\n\nIER_Sim_Data\n\n\n\n  \n\n\n\nCalculate the even-odd consistency score for the personality variables.\n\n\nCode\nIER_Scores&lt;- data.frame(matrix(ncol = 0, nrow = 500))\n\nIER_Scores$EO_score &lt;- evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4))\n\n\nWarning in evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4)): Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n\n\nWarning: The number of items specified by 'factors' does not match the number of columns in 'x'. \n Please check if this is what you want.\n\n\nNow calculate the psychometric synonym scores for ALL of the personality/behavior variables.\nFirst, I create a dataset for the variables. Then, I determine how many item pairs have a correlation greater than 0.60.\n\n\nCode\npsychsyn_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychsyn_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychsyn_cor$cor &gt; .60, na.rm = TRUE)\n\n\n[1] 7\n\n\nIf there are enough pairs, proceed with calculating individual scores.\n\n\nCode\nIER_Scores$psychsyn_score &lt;- psychsyn(IER_Sim_Data, critval = .60)\n\n\nSince positive correlations indicate participants are responding consistently (i.e. carefully), multiply these scores by -1 so that higher scores reflect IER.\n\n\nCode\nIER_Scores$psychsyn_Index &lt;- -1 *(IER_Scores$psychsyn_score)\n\n\nNow calculate the psychometric antonym scores for ALL of the personality/behavior variables.\nFirst, I determine how many item pairs have a correlation less than -0.60.\n\n\nCode\npsychant_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychant_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychant_cor$cor &lt; -.60, na.rm = TRUE)\n\n\n[1] 0\n\n\nIf there are enough pairs, proceed with calculating individual scores. There were 0 in this dataset.\nNext, calculate the IRV index for the personality variables.\nThe intra-individual response variability (IRV) is similiar in spirit to the Longstring index. It is defined as the “standard deviation of responses across a set of consecutive item responses for an individual” (Dunn et al. 2018).\nSince all the items are positively worded, I am calculating this over the whole survey and then splitting the survey into four sections.\n\n\nCode\nIER_Scores$irv_scores &lt;- irv(IER_Sim_Data, split = FALSE)\n\n\nCalculate the long string index for the personality variables.For each observation, the length of the maximum uninterrupted string of identical responses is returned.\n\n\nCode\nIER_Scores$careless_long&lt;-longstring(IER_Sim_Data,avg=FALSE)\n\n\nNow I have to clean up all the NA values in the pyschometric synonym and psychometric antonym indices. NA values index indicate that there was no variance within any of the pairs. Since no variance indicates extreme consistency, and these indices are meant to catch inconsistent respondents. I assign a value of -1 to these respondents because they were responding “carefully,” which really means consistently with these indices.\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$psychsyn_Index)\n\nIER_Scores$psychsyn_Index[missing_rows] &lt;- -1\n\n\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$EO_score)\n\nIER_Scores$EO_score[missing_rows] &lt;- -1\n\n\nCompute Mahalanobis D Index\n\n\nCode\nIER_Scores$mahalscore &lt;- mahad(IER_Sim_Data, plot = FALSE, flag = FALSE, confidence = 0.95, na.rm = TRUE)\n\n\nThe below code converts the IER scores to z-scores.\n\n\nCode\nIER_Sim_Data$ZLongstring_Index &lt;- (IER_Scores$careless_long-mean(IER_Scores$careless_long))/sd(IER_Scores$careless_long)\n\nIER_Sim_Data$ZIRVTotal_Index &lt;- (IER_Scores$irv_scores-mean(IER_Scores$irv_scores))/sd(IER_Scores$irv_scores)\n\nIER_Sim_Data$ZEO_score &lt;- (IER_Scores$EO_score-mean(IER_Scores$EO_score))/sd(IER_Scores$EO_score)\n\nIER_Sim_Data$Zpsychsyn_score &lt;- (IER_Scores$psychsyn_Index-mean(na.omit(IER_Scores$psychsyn_Index)))/sd(na.omit(IER_Scores$psychsyn_Index))\n\nIER_Sim_Data$Zmahal &lt;- (IER_Scores$mahalscore-mean(na.omit(IER_Scores$mahalscore)))/sd(na.omit(IER_Scores$mahalscore))\n\n\nAt this point, we have calculated the scores for all of the IER indices and can move forward the the analyses.",
    "crumbs": [
      "Analyses",
      "Darren"
    ]
  },
  {
    "objectID": "DarrenAnalysis.html#step-one-prepare-the-rstudio-environment",
    "href": "DarrenAnalysis.html#step-one-prepare-the-rstudio-environment",
    "title": "Mallory Analysis",
    "section": "Step One: Prepare the RStudio Environment",
    "text": "Step One: Prepare the RStudio Environment\nThe below code calls the libraries necessary for the technique.\n\n\nCode\nlibrary(haven)\nlibrary(lavaan)\nlibrary(lavaanPlot)",
    "crumbs": [
      "Analyses",
      "Darren"
    ]
  },
  {
    "objectID": "DarrenAnalysis.html#step-two-model-specification",
    "href": "DarrenAnalysis.html#step-two-model-specification",
    "title": "Mallory Analysis",
    "section": "Step Two: Model Specification",
    "text": "Step Two: Model Specification\nThe lavaan model syntax is used to describe models to be estimated. The syntax defines models using various formula types. The formula types which are used in the technique are below.\nLatent Variable Definition:\nlatent variable =~ indicator1 + indicator2 + indicator3\nVariances and Covariances:\nvariable1 ~~ variable2\nRegression:\nvariable1 ~~ variable2 + variable3 + variable4\nBy default, the lavaan syntax will always fix the factor loading of a latent variable's first indicator to 1. Add the argument std.lv = TRUE to the function call to force this factor loading to be free.\n\nBaseline Model\nFirst, you must establish a baseline model that will be used in subsequent models and test. The baseline model is a confirmatory factor analysis (CFA) model of the substantive variables and IER variables. Estimates of the unstandardized factor loadings and error variances for the IER variables are retained from this model to use as fixed parameters in subsequent models (Williams, 2016). Additionally, the factor correlations between the substantive variables are recorded to be used in a later model.\nThe code to specify the baseline for this example is as follows:\n\n\nCode\nBaselineModel &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\nfri =~ ri1 + ri2 + ri3\nfwom =~ wom1 + wom2 + wom3\nfsat =~ sat1 + sat2 + sat3\nfdj =~ dj1 + dj2 + dj3\nfij =~ ij1 + ij2 + ij3 + ij4\nfpj =~ pj1 + pj2 + pj3 + pj4\n\nfIER1 =~ ZLongstring_Index\nfIER2=~ Zpsychsyn_score\nfIER3=~ Zmahal'\n\nBaselineModel_fit &lt;- cfa(BaselineModel, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(BaselineModel_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n803.718 257.000   0.907   0.065 \n\n\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 1\nThe first model to examine is a structural model without IER effects which allows for correlations among the substantive latent variables with an orthogonal IER variable. The IER variable’s indicators are the standardized IER indices retained in the exploratory factor analysis.\nThe IER variable is assumed to be orthogonal, as IER behavior should not be correlated with the substantive variables.\nThe code to specify the Model 1 for this example is as follows:\n\n\nCode\nModel1 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel1_fit &lt;- cfa(Model1, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model1_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n   chisq       df      cfi    rmsea \n1148.016  278.000    0.851    0.079 \n\n\nRecord the substantive factor correlations estimated in this model to use in subsequent models. This can be done by sifting through the lavaan summary output or by utilizing code to extract the estimates from the summary.\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 2\nModel 2 is a saturated structural model with IER effects. With this model, the impact of the IER latent variable is allowed to be different for each of the indicators of the substantive constructs. This is done so that the subsequent test of the presence of IER can be conducted without making any assumptions about the specific nature of these effects.\n\n\nCode\nModel2 &lt;-'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel2_fit &lt;- cfa(Model2, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model2_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n734.846 209.000   0.910   0.071 \n\n\n\n\nModel 3\nModel 3 is identical to Model 2, except that the substantive factor correlations are constrained to their unstandardized estimates from Model 1.\nModel 3 is a restricted saturated structural model with IER effects; the term restricted indicates that the values of the substantively important parameters have been restricted to be equal to the estimates from Model 1. The examination of Model 3 provides a direct statistical test of the impact of IER effects.\nThe code to specify Model 3 for this example is as follows:\n\n\nCode\nModel3 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        \n        fssr ~~ 0.382*fri + 0.936*fwom + 0.311*fsat + 0.805*fdj + 0.602*fij + 0.461*fpj\n        fri ~~ 0.319*fwom + 0.782*fsat + 0.506*fdj + 0.493*fij + 0.547*fpj\n        fwom ~~ 0.260*fsat + 0.692*fdj + 0.655*fij + 0.416*fpj\n        fsat ~~ 0.478*fdj + 0.575*fij + 0.689*fpj\n        fdj ~~ 0.639*fij + 0.586*fpj\n        fij ~~ 0.597*fpj'\n\nModel3_fit &lt;- cfa(Model3, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model3_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n825.349 230.000   0.898   0.072",
    "crumbs": [
      "Analyses",
      "Darren"
    ]
  },
  {
    "objectID": "DarrenAnalysis.html#step-three-compare-the-models",
    "href": "DarrenAnalysis.html#step-three-compare-the-models",
    "title": "Mallory Analysis",
    "section": "Step Three: Compare the models",
    "text": "Step Three: Compare the models\nFirst, a comparison of Model 1 and Model 2 tests the hypothesis that that there are no method effects due to IER.\n\n\nCode\nlavTestLRT(Model1_fit, Model2_fit)\n\n\n\n  \n\n\n\nNext, a comparison of Model 2 and Model 3 tests whether the substantive variable correlations are significantly biased by IER effects.\n\n\nCode\nlavTestLRT(Model3_fit, Model2_fit)\n\n\n\n  \n\n\n\nExport IER Scores\n\n\nCode\n#| echo: false\nlibrary(downloadthis)\nIER_Scores |&gt;\n  download_this(\n    output_name = \"IERScores\",\n    output_extension = \".xlsx\",\n    button_label = \"Download data\",\n    button_type = \"warning\",\n    has_icon = TRUE,\n    icon = \"fa fa-save\"\n  )\n\n\n Download data",
    "crumbs": [
      "Analyses",
      "Darren"
    ]
  },
  {
    "objectID": "Avatars.html",
    "href": "Avatars.html",
    "title": "IER Avatars",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThis is lavaan 0.6-18\nlavaan is FREE software! Please report any bugs.\nTo create simulated insufficient error response data, we will use avatars. Each avatar has a name and an IER pattern.\nWe will also need a survey structure the avatars will fill. We can obtain the structure from the model. However, the structure is not the only part needed. We will also need to set the order of the survey since some avatars have a pattern that depends on the order of the survey. We will also need to add marker variables for CMV.\nFor now we will use the order of the data as the order of the survey and add in a CMV marker variable. We will assume that all of our simulated data is from users who always choose “Strongly Agree” for the CMV marker variable.\nCode\n#get the simulated data\nsim_data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))\n\n\nRows: 500 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (24): ssr1, ssr2, ssr3, ri1, ri2, ri3, wom1, wom2, wom3, sat1, sat2, sat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#add a CMV marker variable that simulates \"Always choose Strongly Agree\", which in this case means it will be 7\n#add it to after wom3\nsim_data &lt;- sim_data |&gt; add_column(cmv1 = 7, .after = \"wom3\")\n\nsim_data",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-1-mallory",
    "href": "Avatars.html#avatar-1-mallory",
    "title": "IER Avatars",
    "section": "Avatar 1: Mallory",
    "text": "Avatar 1: Mallory\nMallory is an accountant at a hotel management company. The HR manager at Mallory’s company is friends with a professor at a local university, so about once every year, Mallory and her coworkers are asked to complete surveys for research studies. As an incentive for participating in a survey when asked, if at least 50% of the company completes the survey, the office closes early the Friday after the survey closes. Most of the surveys ask about work environment and behaviors, including citizenship behaviors, organizational justice, workplace deviance, and job performance. When Mallory completed the first few surveys, she paid close attention to the survey items and answered thoughtfully and honestly, thinking the results of the survey might lead to improvements in the office (and a half-day off work). However, after about five years of taking these surveys, she realized changes weren’t coming, only the half-day off. At that point, her only motivation for participating in the survey was to make sure she did her part to get the survey participation rate up to the goal that was set for the survey. As a result, Mallory has stopped reading the questions in the survey altogether and just selects “neutral” for every item.\n\nGenerate Data\n\n\nCode\n# this function returns one mallory row with the structure of example_data\nmallory &lt;- function(example_data) {example_data[1,] |&gt; mutate(across(everything(), ~4))}\n\nmallory(sim_data)\n\n\n\n  \n\n\n\nWe only need one line of Mallory. To get multiple lines for insertion into a dataset, we can use replicate:\n\n\nCode\n#For example, to get 5 lines of Mallories\nmallories &lt;- function(num_rows, example_data) {replicate(num_rows, mallory(example_data), simplify = FALSE) |&gt; bind_rows()}\nmallories(5, sim_data)\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\n# Note: the histograms function is defined in the setup block of this page\nmallories(1, sim_data) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-2-darren",
    "href": "Avatars.html#avatar-2-darren",
    "title": "IER Avatars",
    "section": "Avatar 2: Darren",
    "text": "Avatar 2: Darren\nDarren is an attorney and works for the same company as Mallory. He’s never been a fan of the surveys because frankly, he feels like he is way too busy for something as silly as academic research. However, everyone hounds him to complete the surveys when they come around because of the promise of a half-day off work if the participation rate for the survey is high enough. He’s worried that if he doesn’t appear to take the survey seriously, his responses won’t be recorded, rendering his time completing the survey completely wasted. So, to appear like he’s thoughtfully responding to the survey, he randomly selects responses to the survey items so that there’s no risk of his responses being scrapped.\n\nGenerate Data\n\n\nCode\n# Darren's pattern is to randomly select a response for each item.  For Darren, we need to know how many we need in advance\n\n# This function will return num_rows darrens with the structure of example_data\ndarren &lt;- function(num_rows, example_data) {\n  1:num_rows |&gt; \n    map(\\(x) example_data[1,] |&gt; mutate(across(everything(), ~sample(1:7, 1)))) |&gt; \n    list_rbind()\n}\n\ndarren(10, sim_data)\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\ndarren(500, sim_data) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-3-bart",
    "href": "Avatars.html#avatar-3-bart",
    "title": "IER Avatars",
    "section": "Avatar 3: Bart",
    "text": "Avatar 3: Bart\nBart is a financial analyst who lives and works in a large metropolitan area. He commutes to work by train every day, and the trip is about 45 minutes each way. His work is stressful and draining, so sometimes he completes surveys online to wind down after a long day. He gets paid to do it, but that isn’t even something he thinks about when selecting surveys to complete. Bart’s “payment” is the enjoyment he gets from completing surveys. He likes to be introspective, likening it to meditation, so completing surveys is fascinating to him. So, when Bart completes a survey, he gives it his full attention. Well, he gives it his full attention for as long as he can. Some surveys are just too long, and he gets fatigued by repetitive questions that seem to go on and on. When that happens, Bart tries to give his brain a break by skimming questions or randomly allowing himself to zone out while reading questions after he’s been in a survey for a while.\n\nComments\nThis is the trickier one. Currently, our survey is too short to simulate Bart. Christie and I had talked about needing to add more variables to our simulation for this. However, Christie mentioned that they have always kept the scales short for our CMV studies because CMV is more likely to be detected that way. Are we going to be washing out the CMV possibility with a longer set of items? My question is—how does a longer list of short scales affect our ability to detect CMV? I’d assume in a similar way, right?\n\n\nGenerate Data\n\n\nCode\n# change the given data to look like a set of Bart responses\nbart &lt;- function(existing_data) {\n  # randomly select about half of the columns to be Bart's random point of zoning out\n  col_num &lt;- length(names(existing_data)) \n  existing_data |&gt; \n    # use row numbers to pivot each respondent into a group\n    mutate (row_num = row_number()) |&gt;\n    pivot_longer(cols = -row_num, names_to = \"question\", values_to = \"response\") |&gt; \n    group_by(row_num) |&gt;\n    # randomly select about half of the columns to be Bart's random point of zoning out\n    group_modify ( ~{\n      about_half &lt;- rnorm(1, col_num/2, col_num/10) |&gt; round()\n      \n      # keep the first half of the columns\n      start &lt;- .x |&gt; slice_head(n = col_num - about_half)\n      \n      # replace the second half of the columns with random responses\n      end &lt;- .x |&gt; slice_tail(n = about_half) |&gt;\n      mutate(response = sample(1:7, about_half, replace = TRUE))\n      \n      # put them back together\n      rbind(start, end) \n    }) |&gt;\n    # put the data back into tabular format\n    pivot_wider(names_from = question, values_from = response) |&gt; ungroup() |&gt; select(-row_num)\n }\n\nbart(slice_sample(sim_data, n = 10))\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\nbart(sim_data) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-4-scarlett",
    "href": "Avatars.html#avatar-4-scarlett",
    "title": "IER Avatars",
    "section": "Avatar 4: Scarlett",
    "text": "Avatar 4: Scarlett\nScarlett is an office manager who spends most of her day at work filing paperwork and ordering office supplies. She has quite a bit of downtime at work, so she completes surveys to earn some extra cash. However, she has often had to rush through completing surveys due to being called into meetings last minute. Scarlett has read on blogs and message boards that researchers often use trap questions and consistency screening methods to catch participants who are rushing or careless, and those participants don’t get paid for their work. So, when she must rush through a survey (whether it’s halfway through a survey or for its entirety), she’s come up with her own system. First, she reads the first question on the page to decide how to answer the rest of the items. Since she doesn’t want to seem too consistent, she varies her answers for the rest of the page around her answer to the first item. Then, while answering the items on the rest of the page in a nonrandom pattern, she pays just enough attention to the survey items to spot catch questions such as, “Please mark STRONGLY AGREE for this item,” or “I am paid biweekly by gremlins,” so that she will answer those items correctly.\n\nComments\nFor this avatar, we’d need the response to the first item on a page to be selected randomly. Then, each subsequent response is a random choice within a constrained range around the first response. For example, if “4” is the response for the first item, subsequent items on the page should be a random selection of 3, 4, or 5.\n\n\nGenerate Data\nTo make this work, we need to indicate which items are on a page.\n\n\nCode\n# We'll create a tibble with each variable and on what page it appears\npage_table &lt;- tribble(\n  ~question, ~page,\n   \"ssr1\",   1,\n   \"ssr2\",   1,\n   \"ssr3\",   1,\n   \"ri1\",    1,\n   \"ri2\",    1,\n   \"ri3\",    1,\n   \"wom1\",   1,\n   \"wom2\",   1,\n   \"wom3\",   1,\n   \"cmv1\",   1,\n   \"sat1\",   2,\n   \"sat2\",   2,\n   \"sat3\",   2,\n   \"dj1\",    2,\n   \"dj2\",    2,\n   \"dj3\",    2,\n   \"dj4\",    2,\n   \"ij1\",    3,\n   \"ij2\",    3,\n   \"ij3\",    3,\n   \"ij4\",    3,\n   \"pj1\",    3,\n   \"pj2\",    3,\n   \"pj3\",    3,\n   \"pj4\",    3\n)\n\n# This function will return num_rows scarletts with the structure of example_data and the page structure abov\nscarlett &lt;- function(num_rows, page_table) {\n  1:num_rows |&gt; \n    map(\\(x) {\n      page_table |&gt; group_by(page) |&gt; \n        group_modify(~{\n          # give every question on the page the same value\n          g &lt;- .x |&gt; mutate(value = sample(1:7, 1))\n          # create a vector of normally distributed adjustments for the all questions \n          # with the first being zero\n          adjustments &lt;- c(0, rnorm(nrow(g) - 1, 0, 1) |&gt; round())\n          # adjust each value by the random amount\n          g |&gt; mutate(value = value + adjustments) |&gt;\n            # make sure the values are between 1 and 7\n            mutate(value = pmax(1, pmin(7, value)))\n        }) |&gt; ungroup() |&gt; select(-page) |&gt;\n        # put into tabular format\n        pivot_wider(names_from = question, values_from = value) \n    }) |&gt;\n    list_rbind()\n}\n\nscarlett(10, page_table)\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\nscarlett(500, page_table) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html",
    "href": "GenerateBaselineData.html",
    "title": "GenerateBaselineData",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThis is lavaan 0.6-18\nlavaan is FREE software! Please report any bugs.",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#introduction",
    "href": "GenerateBaselineData.html#introduction",
    "title": "GenerateBaselineData",
    "section": "Introduction",
    "text": "Introduction\nIn this document we generate a random dataset using the model first introduced in (Maxham and Netemeyer 2003) and used for a CMV simulation in (Fuller et al. 2016)",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#the-model",
    "href": "GenerateBaselineData.html#the-model",
    "title": "GenerateBaselineData",
    "section": "The model",
    "text": "The model\nThe model we will use has the constructs in Table 1 below\n\n\n\nConstruct\nAbbreviation\nNumber of measures\n\n\n\n\nSatisfaction with Recovery\nSSR\n3\n\n\nReturn Intent1\nRI\n3\n\n\nWord of Mouth\nWOM\n3\n\n\nSatisfaction\nSAT\n3\n\n\nDistributive Justice\nDJ\n4\n\n\nInteractional Justice\nIJ\n4\n\n\nProcedural Justice\nPJ\n4\n\n\n\nEach of these constructs has several measures, and the model is expressed as a confirmatory factor analysis in the lavaan equation below and associated diagram\n\n\nCode\ncfa_sim_model &lt;- \"\n  # Measurement model\n  SSR =~ ssr1 + ssr2 + ssr3\n  RI  =~ ri1 + ri2 + ri3\n  WOM =~ wom1 + wom2 + wom3\n  SAT =~ sat1 + sat2 + sat3\n  DJ  =~ dj1 + dj2 + dj3 + dj4\n  IJ  =~ ij1 + ij2 + ij3 + ij4\n  PJ  =~ pj1 + pj2 + pj3 + pj4\n\n  # Variances\n  SSR ~~ SSR\n  RI ~~ RI\n  WOM ~~ WOM\n  SAT ~~ SAT\n  DJ ~~ DJ\n  IJ ~~ IJ\n  PJ ~~ PJ\n  \n  # Covariances\n  SSR ~~ RI  \n  SSR ~~ WOM \n  SSR ~~ SAT \n  SSR ~~ DJ  \n  SSR ~~ IJ  \n  SSR ~~ PJ  \n  RI ~~ WOM \n  RI ~~ SAT \n  RI ~~ DJ  \n  RI ~~ IJ  \n  RI ~~ PJ  \n  WOM ~~ SAT \n  WOM ~~ DJ  \n  WOM ~~ IJ  \n  WOM ~~ PJ  \n  SAT ~~ DJ  \n  SAT ~~ IJ  \n  SAT ~~ PJ  \n  DJ ~~ IJ  \n  DJ ~~ PJ  \n  IJ ~~ PJ\n  \n  # Means\n  ssr1 ~ ssr_int * 1\n  ssr2 ~ ssr_int * 1\n  ssr3 ~ ssr_int * 1 \n  ri1 ~ ri_int * 1\n  ri2 ~ ri_int * 1\n  ri3 ~ ri_int * 1\n  wom1 ~ wom_int * 1\n  wom2 ~ wom_int * 1\n  wom3 ~ wom_int * 1\n  sat1 ~ sat_int * 1\n  sat2 ~ sat_int * 1\n  sat3 ~ sat_int * 1\n  dj1 ~ dj_int * 1\n  dj2 ~ dj_int * 1\n  dj3 ~ dj_int * 1\n  dj4 ~ dj_int * 1\n  ij1 ~ ij_int * 1\n  ij2 ~ ij_int * 1\n  ij3 ~ ij_int * 1\n  ij4 ~ ij_int * 1\n  pj1 ~ pj_int * 1\n  pj2 ~ pj_int * 1\n  pj3 ~ pj_int * 1\n  pj4 ~ pj_int * 1\n\"\nsemPlotModel_lavaanModel(cfa_sim_model) |&gt; \n  semPaths(layout=\"tree2\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), \n           label.norm = \"OOOOO\", intAtSide = TRUE, levels = c(1,2,7,8), curvature = 2, \n           residuals = FALSE)",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#the-data",
    "href": "GenerateBaselineData.html#the-data",
    "title": "GenerateBaselineData",
    "section": "The Data",
    "text": "The Data\nLet’s use the correlation matrix data from (Fuller et al. 2016) to fit this model\n\n\nCode\n# enter as a lower triangular matrix\nlower_tri &lt;- matrix(0, ncol=7, nrow=7)\n\n# Fill the columns of the lower triangle\nlower_tri[1:1, 1] &lt;- .88\nlower_tri[1:2, 2] &lt;- c(.46, .88)\nlower_tri[1:3, 3] &lt;- c(.70, .48, .88)\nlower_tri[1:4, 4] &lt;- c(.38, .65, .34, .90)\nlower_tri[1:5, 5] &lt;- c(.64, .50, .53, .51, .89)\nlower_tri[1:6, 6] &lt;- c(.55, .47, .57, .52, .57, .89)\nlower_tri[1:7, 7] &lt;- c(.47, .44, .43, .59, .55, .55, .87)\n\n# Create the full correlation matrix\ncor_matrix &lt;- lower_tri + t(lower_tri) - diag(diag(lower_tri))\n\n# get the reliabilities\nreliabilities &lt;- diag(cor_matrix)\n\n# set the diagonal to 1\ndiag(cor_matrix) &lt;- 1\n\ncor_matrix\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,] 1.00 0.46 0.70 0.38 0.64 0.55 0.47\n[2,] 0.46 1.00 0.48 0.65 0.50 0.47 0.44\n[3,] 0.70 0.48 1.00 0.34 0.53 0.57 0.43\n[4,] 0.38 0.65 0.34 1.00 0.51 0.52 0.59\n[5,] 0.64 0.50 0.53 0.51 1.00 0.57 0.55\n[6,] 0.55 0.47 0.57 0.52 0.57 1.00 0.55\n[7,] 0.47 0.44 0.43 0.59 0.55 0.55 1.00\n\n\nThese are only the correlations. We need to use the standard deviations to get the variance-covariance matrix. At the same time let’s add the names and set the means.\n\n\nCode\nvar_names &lt;- c(\"ssr\", \"ri\", \"wom\", \"sat\", \"dj\", \"ij\", \"pj\")\n\n# Assign the names to the correlation matrix\nrownames(cor_matrix) &lt;- var_names\ncolnames(cor_matrix) &lt;- var_names\n\n# Add in the means and standard deviation:\nconstruct_means &lt;- c(SSR = 5.49, RI = 3.25, WOM = 4.03, SAT = 4.27, DJ = 3.44, IJ = 3.89, PJ = 3.93)\nconstruct_sds &lt;- c(SSR = 1.01, RI = 1.03, WOM = 0.92, SAT = 1.45, DJ = 1.38, IJ = 1.27, PJ = 1.17)\n\n#create the covariance matrix\ncov_matrix &lt;- cor2cov(cor_matrix, construct_sds) \ncov_matrix\n\n\n         ssr       ri      wom      sat       dj       ij       pj\nssr 1.020100 0.478538 0.650440 0.556510 0.892032 0.705485 0.555399\nri  0.478538 1.060900 0.454848 0.970775 0.710700 0.614807 0.530244\nwom 0.650440 0.454848 0.846400 0.453560 0.672888 0.665988 0.462852\nsat 0.556510 0.970775 0.453560 2.102500 1.020510 0.957580 1.000935\ndj  0.892032 0.710700 0.672888 1.020510 1.904400 0.998982 0.888030\nij  0.705485 0.614807 0.665988 0.957580 0.998982 1.612900 0.817245\npj  0.555399 0.530244 0.462852 1.000935 0.888030 0.817245 1.368900",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#simulating-new-data",
    "href": "GenerateBaselineData.html#simulating-new-data",
    "title": "GenerateBaselineData",
    "section": "Simulating New Data",
    "text": "Simulating New Data\nTo simulate data we need all of the right-side components of the following:\n\n\\mathbf{y_j}=\\mathbf{\\Lambda}\\mathbf\\eta_{j} + \\mathbf{\\epsilon_{j}}\n\nThe notation is as follows:\n\ny_{ij} is the response of person j\\,(j=1,...,J) on item i\\,(i=1,...,I).\n\\eta_{jk} is the kth common factor for person j.\n\\lambda_{ik} is the factor loading of item i on factor k.\n\\epsilon_{ij} is the random error term for person j on item i.\n\\mathbf{\\Psi} is the variance-covariance matrix of the common factors η_j.\n\\mathbf{\\Theta} is the variance-covariance matrix of the residuals (or unique factors) ϵ_j.\n\nIn expanded format\n\n\\underbrace{\\left[\\begin{array}{l}\ny_{1 j} \\\\\ny_{2 j} \\\\\n\\vdots \\\\\ny_{i j}\n\\end{array}\\right]}_{\\mathbf{y}_{j}}=\\underbrace{\\left[\\begin{array}{c}\n\\lambda_{11} & & 0 \\\\\n\\lambda_{21} & & 0 \\\\\n\\vdots & \\cdots & \\vdots \\\\\n\\lambda_{i1} & & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 &  & \\lambda_{ik} \\\\\n\\vdots & \\cdots & \\vdots \\\\\n0 & & \\lambda_{ik}\n\\end{array}\\right]}_{\\Lambda}\\underbrace{\\left[\\begin{array}{l}\n\\eta_{1 j} \\\\\n\\eta_{2 j} \\\\\n\\vdots \\\\\n\\eta_{i j}\n\\end{array}\\right]}_{\\mathbf{\\eta}_j}+\\underbrace{\\left[\\begin{array}{c}\n\\epsilon_{1 j} \\\\\n\\epsilon_{2 j} \\\\\n\\vdots \\\\\n\\epsilon_{i j}\n\\end{array}\\right]}_{\\mathbf{\\epsilon}_j}\n\nWhere\n\n\\mathbf{\\epsilon}_{j} \\sim N_{I}(\\mathbf{0}, \\mathbf{\\Theta})\n\n\n\\mathbf{\\eta}_{j} \\sim N_{K}(\\mathbf{\\beta}, \\mathbf{\\Psi}),\n Where \\mathbf{\\beta}_k is the mean of each construct.\nAnd\n\n\\mathbf{\\Psi}=\\mathrm{Cov}\\begin{pmatrix}\n\\eta_{1j} \\\\\n\\eta_{2j} \\\\\n\\vdots \\\\\nn_{ij}\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\psi_{11} & \\psi_{12} & \\cdots & \\psi_{1k} \\\\\n\\psi_{21} & \\psi_{22} & \\cdots & \\psi_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\psi_{i1} & \\psi_{i2} & \\cdots & \\psi_{ik}\n\\end{pmatrix}\n\n\n\\mathbf{\\Theta}=\\mathrm{Cov}\n\\begin{pmatrix}\n\\epsilon_{1 j} \\\\\n\\epsilon_{2 j} \\\\\n\\epsilon_{3 j} \\\\\n\\vdots \\\\\n\\epsilon_{i j}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta_{11} & 0 & \\cdots & 0 \\\\\n0 & \\theta_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\theta_{ii}\n\\end{pmatrix}\n\nThe covariance matrix is \\mathbf{\\Psi}, which we derived from the given correlation matrix.\n\n\nCode\npsi &lt;- cov_matrix\n\n\nFor \\mathbf{\\Theta} we need to determine the error of each item. Since we only have there reliability for each construct, we’ll need to derive each \\theta_{ii}, or the covariance for each item. We are given the reliability of each construct R_{kk}, which is an estimate of the correlation coefficient of each construct. The covariance \\epsilon_k, then, for each construct is (1 - R_{kk})^2. The linear combination of the covariance of each item x_i in construct k is \\epsilon_k = \\frac{\\sum_1^ix_{i}}{N_i}. If x_1 = x_2 ...= x_i, then \\epsilon_k = \\frac{N_ix}{N_i} = x. Therefore, each \\theta_{ii} = (1 - R_{kk})^2 for each i in each k.\nSo, for each construct we will use one minus its reliability for the covariance of its error term to construct \\mathbf{\\Theta} .\n\n\nCode\nitems_per_construct &lt;- c(3, 3, 3, 3, 4, 4, 4)\nsigma_k &lt;- (1 - reliabilities)^2 #construct error variance\ntheta &lt;- sigma_k |&gt; \n  rep(items_per_construct) |&gt; \n  diag(sum(items_per_construct), sum(items_per_construct))\n\n\nWe know the average loading for each item on each construct, but we don’t know what each item loading is exactly. For now we will have each loading be 1. But we could do the following to simulate \\mathbf{\\Lambda}:\nIn measurement theory, the reliability r_{11} is defined as r_{11} = \\frac{\\sigma^2_t}{\\sigma^2_x} where t is the true factor score and x is the measured score(Nunnally and Bernstein 1994, eq. 6-9) . Therefore, \\sigma_t^2 = r_{11}\\sigma_x^2. \\sigma_x^2 is represented by \\mathbf{\\Psi}, so \\mathbf{\\Lambda} can be represented by the reliability for each construct r_{kk}\n\n\nCode\n# Create a matrix filled with zeros with rows equal to the total number of items\n# and columns equal to the number of factors\n\n#TODO change zeros to normally random loadings \nlambda &lt;- matrix(0, nrow = sum(items_per_construct), ncol = length(items_per_construct))\n\n# Fill the matrix with the 1s where each factor loads onto the corresponding items\nstart_index &lt;- 1\nfor (i in seq_along(items_per_construct)) {\n  end_index &lt;- start_index + items_per_construct[i] - 1\n  lambda[start_index:end_index, i] &lt;- reliabilities[i]\n  start_index &lt;- end_index + 1\n}\n\n\nWe can now use all of the components to generate a data set\n\n\nCode\nJ &lt;- 500\n\n# generate random factor scores for each construct on each subject\neta &lt;- MASS::mvrnorm(J, mu = construct_means, Sigma = psi)\n\n# generate error terms for each item for each subject\nepsilon &lt;- MASS::mvrnorm(J, mu = rep(0, ncol(theta)), Sigma = theta)\n\n\nY &lt;- tcrossprod(eta, lambda) + epsilon \n\nY &lt;- Y |&gt; round() # turn in to likert.  Should already be normally distributed\nY[Y &lt;= 0] &lt;- 1 #capture those that end up just below 0.5 and just above 7.5\nY[Y &gt;= 8] &lt;- 7\n\nitem_names &lt;- mapply(\\(name, num) \n                     paste0(name, seq(num)), var_names, items_per_construct, SIMPLIFY = FALSE) |&gt;\n  unlist()\n\ncfa_data  &lt;-  Y |&gt; as.data.frame() |&gt; setNames(item_names)\ncfa_data\n\n\n\n  \n\n\n\nPlot each variable to check if it is reasonable\n\n\nCode\ncfa_data |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  ggplot(aes(value)) +\n  geom_histogram(binwidth = .5) +\n  scale_x_continuous(breaks = seq(1, 7, 1), limits = c(0.5, 7.5)) +\n  facet_wrap(vars(name))\n\n\n\n\n\n\n\n\n\nSave the simulated data to allow for later use.\n\n\nCode\ncfa_data |&gt; readr::write_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#download-the-data",
    "href": "GenerateBaselineData.html#download-the-data",
    "title": "GenerateBaselineData",
    "section": "Download the Data",
    "text": "Download the Data\nClick on the button to download the data\n\n\n Download data",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#check-the-simulated-data",
    "href": "GenerateBaselineData.html#check-the-simulated-data",
    "title": "GenerateBaselineData",
    "section": "Check the Simulated Data",
    "text": "Check the Simulated Data\nLet’s see what the results are when we try to fit the data to the model\n\n\nCode\nfit&lt;- cfa(cfa_sim_model, cfa_data, meanstructure = TRUE)\nsummary(fit)\n\n\nlavaan 0.6-18 ended normally after 186 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        93\n  Number of equality constraints                    17\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               228.552\n  Degrees of freedom                               248\n  P-value (Chi-square)                           0.807\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  SSR =~                                              \n    ssr1              1.000                           \n    ssr2              0.996    0.018   54.588    0.000\n    ssr3              1.001    0.019   52.101    0.000\n  RI =~                                               \n    ri1               1.000                           \n    ri2               1.026    0.020   52.242    0.000\n    ri3               1.012    0.019   51.926    0.000\n  WOM =~                                              \n    wom1              1.000                           \n    wom2              0.990    0.022   45.692    0.000\n    wom3              0.978    0.021   45.999    0.000\n  SAT =~                                              \n    sat1              1.000                           \n    sat2              0.986    0.012   84.325    0.000\n    sat3              0.987    0.011   91.751    0.000\n  DJ =~                                               \n    dj1               1.000                           \n    dj2               1.016    0.012   81.315    0.000\n    dj3               0.995    0.013   76.804    0.000\n    dj4               1.012    0.013   79.722    0.000\n  IJ =~                                               \n    ij1               1.000                           \n    ij2               1.003    0.014   71.530    0.000\n    ij3               1.015    0.014   71.102    0.000\n    ij4               1.032    0.014   72.309    0.000\n  PJ =~                                               \n    pj1               1.000                           \n    pj2               0.987    0.016   60.292    0.000\n    pj3               0.989    0.016   62.188    0.000\n    pj4               1.019    0.016   65.079    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  SSR ~~                                              \n    RI                0.376    0.042    8.900    0.000\n    WOM               0.540    0.044   12.361    0.000\n    SAT               0.429    0.059    7.253    0.000\n    DJ                0.712    0.060   11.771    0.000\n    IJ                0.588    0.057   10.277    0.000\n    PJ                0.439    0.049    8.909    0.000\n  RI ~~                                               \n    WOM               0.329    0.038    8.722    0.000\n    SAT               0.760    0.065   11.743    0.000\n    DJ                0.553    0.056    9.956    0.000\n    IJ                0.501    0.054    9.260    0.000\n    PJ                0.422    0.048    8.811    0.000\n  WOM ~~                                              \n    SAT               0.368    0.053    6.973    0.000\n    DJ                0.514    0.051   10.029    0.000\n    IJ                0.548    0.052   10.594    0.000\n    PJ                0.386    0.044    8.775    0.000\n  SAT ~~                                              \n    DJ                0.751    0.079    9.472    0.000\n    IJ                0.878    0.082   10.757    0.000\n    PJ                0.854    0.075   11.461    0.000\n  DJ ~~                                               \n    IJ                0.828    0.075   11.075    0.000\n    PJ                0.708    0.066   10.718    0.000\n  IJ ~~                                               \n    PJ                0.727    0.066   10.988    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ssr1    (ssr_)    4.833    0.042  116.017    0.000\n   .ssr2    (ssr_)    4.833    0.042  116.017    0.000\n   .ssr3    (ssr_)    4.833    0.042  116.017    0.000\n   .ri1     (r_nt)    2.875    0.041   69.899    0.000\n   .ri2     (r_nt)    2.875    0.041   69.899    0.000\n   .ri3     (r_nt)    2.875    0.041   69.899    0.000\n   .wom1    (wm_n)    3.515    0.037   95.306    0.000\n   .wom2    (wm_n)    3.515    0.037   95.306    0.000\n   .wom3    (wm_n)    3.515    0.037   95.306    0.000\n   .sat1    (st_n)    3.887    0.059   66.390    0.000\n   .sat2    (st_n)    3.887    0.059   66.390    0.000\n   .sat3    (st_n)    3.887    0.059   66.390    0.000\n   .dj1     (dj_n)    3.045    0.054   56.730    0.000\n   .dj2     (dj_n)    3.045    0.054   56.730    0.000\n   .dj3     (dj_n)    3.045    0.054   56.730    0.000\n   .dj4     (dj_n)    3.045    0.054   56.730    0.000\n   .ij1     (ij_n)    3.467    0.053   64.868    0.000\n   .ij2     (ij_n)    3.467    0.053   64.868    0.000\n   .ij3     (ij_n)    3.467    0.053   64.868    0.000\n   .ij4     (ij_n)    3.467    0.053   64.868    0.000\n   .pj1     (pj_n)    3.400    0.047   72.003    0.000\n   .pj2     (pj_n)    3.400    0.047   72.003    0.000\n   .pj3     (pj_n)    3.400    0.047   72.003    0.000\n   .pj4     (pj_n)    3.400    0.047   72.003    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    SSR               0.857    0.059   14.513    0.000\n    RI                0.812    0.056   14.551    0.000\n    WOM               0.679    0.048   14.222    0.000\n    SAT               1.748    0.113   15.437    0.000\n    DJ                1.435    0.094   15.256    0.000\n    IJ                1.408    0.094   15.034    0.000\n    PJ                1.119    0.075   15.007    0.000\n   .ssr1              0.076    0.007   10.904    0.000\n   .ssr2              0.057    0.006    9.287    0.000\n   .ssr3              0.073    0.007   10.620    0.000\n   .ri1               0.069    0.007   10.196    0.000\n   .ri2               0.072    0.007   10.133    0.000\n   .ri3               0.072    0.007   10.295    0.000\n   .wom1              0.075    0.007   10.427    0.000\n   .wom2              0.072    0.007   10.368    0.000\n   .wom3              0.068    0.007   10.187    0.000\n   .sat1              0.042    0.005    8.117    0.000\n   .sat2              0.075    0.006   11.729    0.000\n   .sat3              0.057    0.006   10.128    0.000\n   .dj1               0.052    0.005   11.315    0.000\n   .dj2               0.056    0.005   11.465    0.000\n   .dj3               0.066    0.005   12.337    0.000\n   .dj4               0.060    0.005   11.800    0.000\n   .ij1               0.073    0.006   12.295    0.000\n   .ij2               0.062    0.005   11.581    0.000\n   .ij3               0.065    0.006   11.695    0.000\n   .ij4               0.062    0.005   11.363    0.000\n   .pj1               0.060    0.006   10.861    0.000\n   .pj2               0.087    0.007   12.532    0.000\n   .pj3               0.078    0.006   12.141    0.000\n   .pj4               0.070    0.006   11.435    0.000",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#footnotes",
    "href": "GenerateBaselineData.html#footnotes",
    "title": "GenerateBaselineData",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn (Maxham and Netemeyer 2003), There’s a “Purchase Intent”, but not a “Return Intent”.↩︎",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CMV-IER Simulation Main",
    "section": "",
    "text": "This is a set of pages that run through simulating and testing data for the CMV IER project by Elizabeth Raglund, Christie Fuller, Marcia Simmering, and Doug Twitchell.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#cmv-ier-simulation",
    "href": "index.html#cmv-ier-simulation",
    "title": "CMV-IER Simulation Main",
    "section": "",
    "text": "This is a set of pages that run through simulating and testing data for the CMV IER project by Elizabeth Raglund, Christie Fuller, Marcia Simmering, and Doug Twitchell.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#generate-baseline-data",
    "href": "index.html#generate-baseline-data",
    "title": "CMV-IER Simulation Main",
    "section": "Generate Baseline Data",
    "text": "Generate Baseline Data\nIn this simulation, we generate a data set with no CMV or IER. This is the baseline data set that we will use to compare to the other data sets.\nGenerate Baseline Data",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#investigate-ier-and-cmv-interplay",
    "href": "index.html#investigate-ier-and-cmv-interplay",
    "title": "CMV-IER Simulation Main",
    "section": "Investigate IER and CMV Interplay",
    "text": "Investigate IER and CMV Interplay\nIER CMV Interplay",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#generate-data-from-ier-avatars",
    "href": "index.html#generate-data-from-ier-avatars",
    "title": "CMV-IER Simulation Main",
    "section": "Generate Data from IER Avatars",
    "text": "Generate Data from IER Avatars\nGenerate Data from IER Avatars",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#analyze-each-avatar",
    "href": "index.html#analyze-each-avatar",
    "title": "CMV-IER Simulation Main",
    "section": "Analyze each Avatar",
    "text": "Analyze each Avatar\nMallory\nDarren\nBart\nScarlett",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "IER-CMVInterplay.html",
    "href": "IER-CMVInterplay.html",
    "title": "The Interplay of IER and CMV in Data",
    "section": "",
    "text": "List of all the libraries to use for the analysis\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(foreign)\nlibrary(careless)\nlibrary(haven)\nlibrary(readxl)\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCall the dataset\n\n\nCode\nIER_Sim_Data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))\n\n\nRows: 500 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (24): ssr1, ssr2, ssr3, ri1, ri2, ri3, wom1, wom2, wom3, sat1, sat2, sat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCalculate the even-odd consistency score for the personality variables.\n\n\nCode\nIER_Scores&lt;- data.frame(matrix(ncol = 0, nrow = 500))\n\nIER_Scores$EO_score &lt;- evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4))\n\n\nWarning in evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4)): Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n\n\nNow calculate the psychometric synonym scores for ALL of the personality/behavior variables.\nFirst, I create a dataset for the variables. Then, I determine how many item pairs have a correlation greater than 0.60.\n\n\nCode\npsychsyn_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychsyn_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychsyn_cor$cor &gt; .60, na.rm = TRUE)\n\n\n[1] 55\n\n\nIf there are enough pairs, proceed with calculating individual scores. (there were 49)\n\n\nCode\nIER_Scores$psychsyn_score &lt;- psychsyn(IER_Sim_Data, critval = .60)\n\n\nSince positive correlations indicate participants are responding consistently (i.e. carefully), multiply these scores by -1 so that higher scores reflect IER.\n\n\nCode\nIER_Scores$psychsyn_Index &lt;- -1 *(IER_Scores$psychsyn_score)\n\n\nNow calculate the psychometric antonym scores for ALL of the personality/behavior variables.\nFirst, I determine how many item pairs have a correlation less than -0.60.\n\n\nCode\npsychant_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychant_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychant_cor$cor &lt; -.60, na.rm = TRUE)\n\n\n[1] 0\n\n\nIf there are enough pairs, proceed with calculating individual scores. There were 0 in this dataset.\nNext, calculate the IRV index for the personality variables.\nThe intra-individual response variability (IRV) is similiar in spirit to the Longstring index. It is defined as the “standard deviation of responses across a set of consecutive item responses for an individual” (Dunn et al. 2018).\nSince all the items are positively worded, I am calculating this over the whole survey and then splitting the survey into four sections.\n\n\nCode\nIER_Scores$irv_scores &lt;- irv(IER_Sim_Data, split = FALSE)\n\n\nCalculate the long string index for the personality variables.For each observation, the length of the maximum uninterrupted string of identical responses is returned.\n\n\nCode\nIER_Scores$careless_long&lt;-longstring(IER_Sim_Data,avg=FALSE)\n\n\nNow I have to clean up all the NA values in the pyschometric synonym and psychometric antonym indices. NA values index indicate that there was no variance within any of the pairs. Since no variance indicates extreme consistency, and these indices are meant to catch inconsistent respondents. I assign a value of -1 to these respondents because they were responding “carefully,” which really means consistently with these indices.\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$psychsyn_Index)\n\nIER_Scores$psychsyn_Index[missing_rows] &lt;- -1\n\n\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$EO_score)\n\nIER_Scores$EO_score[missing_rows] &lt;- -1\n\n\nCompute Mahalanobis D Index\n\n\nCode\nIER_Scores$mahalscore &lt;- mahad(IER_Sim_Data, plot = FALSE, flag = FALSE, confidence = 0.95, na.rm = TRUE)\n\n\nThe below code converts the IER scores to z-scores.\n\n\nCode\nIER_Sim_Data$ZLongstring_Index &lt;- (IER_Scores$careless_long-mean(IER_Scores$careless_long))/sd(IER_Scores$careless_long)\n\nIER_Sim_Data$ZIRVTotal_Index &lt;- (IER_Scores$irv_scores-mean(IER_Scores$irv_scores))/sd(IER_Scores$irv_scores)\n\nIER_Sim_Data$ZEO_score &lt;- (IER_Scores$EO_score-mean(IER_Scores$EO_score))/sd(IER_Scores$EO_score)\n\nIER_Sim_Data$Zpsychsyn_score &lt;- (IER_Scores$psychsyn_Index-mean(na.omit(IER_Scores$psychsyn_Index)))/sd(na.omit(IER_Scores$psychsyn_Index))\n\nIER_Sim_Data$Zmahal &lt;- (IER_Scores$mahalscore-mean(na.omit(IER_Scores$mahalscore)))/sd(na.omit(IER_Scores$mahalscore))\n\n\nAt this point, we have calculated the scores for all of the IER indices and can move forward the the analyses.",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "IER-CMVInterplay.html#step-one-prepare-the-rstudio-environment",
    "href": "IER-CMVInterplay.html#step-one-prepare-the-rstudio-environment",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step One: Prepare the RStudio Environment",
    "text": "Step One: Prepare the RStudio Environment\nThe below code calls the libraries necessary for the technique.\n\n\nCode\nlibrary(haven)\nlibrary(lavaan)\nlibrary(lavaanPlot)",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "IER-CMVInterplay.html#step-two-model-specification",
    "href": "IER-CMVInterplay.html#step-two-model-specification",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Two: Model Specification",
    "text": "Step Two: Model Specification\nThe lavaan model syntax is used to describe models to be estimated. The syntax defines models using various formula types. The formula types which are used in the technique are below.\nLatent Variable Definition:\nlatent variable =~ indicator1 + indicator2 + indicator3\nVariances and Covariances:\nvariable1 ~~ variable2\nRegression:\nvariable1 ~~ variable2 + variable3 + variable4\nBy default, the lavaan syntax will always fix the factor loading of a latent variable's first indicator to 1. Add the argument std.lv = TRUE to the function call to force this factor loading to be free.\n\nBaseline Model\nFirst, you must establish a baseline model that will be used in subsequent models and test. The baseline model is a confirmatory factor analysis (CFA) model of the substantive variables and IER variables. Estimates of the unstandardized factor loadings and error variances for the IER variables are retained from this model to use as fixed parameters in subsequent models (Williams, 2016). Additionally, the factor correlations between the substantive variables are recorded to be used in a later model.\nThe code to specify the baseline for this example is as follows:\n\n\nCode\nBaselineModel &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal'\n\nBaselineModel_fit &lt;- cfa(BaselineModel, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(BaselineModel_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n232.488 257.000   1.000   0.000 \n\n\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 1\nThe first model to examine is a structural model without IER effects which allows for correlations among the substantive latent variables with an orthogonal IER variable. The IER variable’s indicators are the standardized IER indices retained in the exploratory factor analysis.\nThe IER variable is assumed to be orthogonal, as IER behavior should not be correlated with the substantive variables.\nThe code to specify the Model 1 for this example is as follows:\n\n\nCode\nModel1 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel1_fit &lt;- cfa(Model1, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model1_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n662.195 278.000   0.982   0.053 \n\n\nRecord the substantive factor correlations estimated in this model to use in subsequent models. This can be done by sifting through the lavaan summary output or by utilizing code to extract the estimates from the summary.\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 2\nModel 2 is a saturated structural model with IER effects. With this model, the impact of the IER latent variable is allowed to be different for each of the indicators of the substantive constructs. This is done so that the subsequent test of the presence of IER can be conducted without making any assumptions about the specific nature of these effects.\n\n\nCode\nModel2 &lt;-'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel2_fit &lt;- cfa(Model2, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model2_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n180.373 209.000   1.000   0.000 \n\n\n\n\nModel 3\nModel 3 is identical to Model 2, except that the substantive factor correlations are constrained to their unstandardized estimates from Model 1.\nModel 3 is a restricted saturated structural model with IER effects; the term restricted indicates that the values of the substantively important parameters have been restricted to be equal to the estimates from Model 1. The examination of Model 3 provides a direct statistical test of the impact of IER effects.\nThe code to specify Model 3 for this example is as follows:\n\n\nCode\nModel3 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        \n        fssr ~~ 0.382*fri + 0.936*fwom + 0.311*fsat + 0.805*fdj + 0.602*fij + 0.461*fpj\n        fri ~~ 0.319*fwom + 0.782*fsat + 0.506*fdj + 0.493*fij + 0.547*fpj\n        fwom ~~ 0.260*fsat + 0.692*fdj + 0.655*fij + 0.416*fpj\n        fsat ~~ 0.478*fdj + 0.575*fij + 0.689*fpj\n        fdj ~~ 0.639*fij + 0.586*fpj\n        fij ~~ 0.597*fpj'\n\nModel3_fit &lt;- cfa(Model3, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model3_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n808.536 230.000   0.973   0.071",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "IER-CMVInterplay.html#step-three-compare-the-models",
    "href": "IER-CMVInterplay.html#step-three-compare-the-models",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Three: Compare the models",
    "text": "Step Three: Compare the models\nFirst, a comparison of Model 1 and Model 2 tests the hypothesis that that there are no method effects due to IER.\n\n\nCode\nlavTestLRT(Model1_fit, Model2_fit)\n\n\n\n  \n\n\n\nNext, a comparison of Model 2 and Model 3 tests whether the substantive variable correlations are significantly biased by IER effects.\n\n\nCode\nlavTestLRT(Model3_fit, Model2_fit)\n\n\n\n  \n\n\n\nExport IER Scores\n\n\n Download data",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "MalloryAnalysis.html",
    "href": "MalloryAnalysis.html",
    "title": "Mallory Analysis",
    "section": "",
    "text": "List of all the libraries to use for the analysis\n\n\nCode\nlibrary(tidyverse)\nlibrary(foreign)\nlibrary(careless)\nlibrary(haven)\nlibrary(readxl)\nlibrary(car)\n\n\nGet the baseline data and replace 20% with Mallories\n\n\nCode\n# get the baseline data\nexample_data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))\n\n\nRows: 500 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (24): ssr1, ssr2, ssr3, ri1, ri2, ri3, wom1, wom2, wom3, sat1, sat2, sat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# define the mallory function\nmallory &lt;- function(example_data) {example_data[1,] |&gt; mutate(across(everything(), ~4))}\n\n# generate many mallories\nmallories &lt;- function(num_rows, example_data) {replicate(num_rows, mallory(example_data), simplify = FALSE) |&gt; bind_rows()}\n\n# generate data with 20% mallories\nIER_Sim_Data &lt;- mallories(nrow(example_data) * 0.2, example_data) |&gt; #generate mallories\n  bind_rows(slice_sample(example_data, n = nrow(example_data) * 0.8)) %&gt;%  # combine with 80% of the original data\n  slice_sample(n = nrow(.)) # shuffle the data\n\nIER_Sim_Data\n\n\n\n  \n\n\n\nCalculate the even-odd consistency score for the personality variables.\n\n\nCode\nIER_Scores&lt;- data.frame(matrix(ncol = 0, nrow = 500))\n\nIER_Scores$EO_score &lt;- evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4))\n\n\nWarning in evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4)): Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n\n\nWarning: One or more observations have zero variance in even and/or odd values. \nThis results in NA values for these observations.\nIncluding more factors may alleviate this issue.\n\n\nNow calculate the psychometric synonym scores for ALL of the personality/behavior variables.\nFirst, I create a dataset for the variables. Then, I determine how many item pairs have a correlation greater than 0.60.\n\n\nCode\npsychsyn_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychsyn_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychsyn_cor$cor &gt; .60, na.rm = TRUE)\n\n\n[1] 38\n\n\nIf there are enough pairs, proceed with calculating individual scores.\n\n\nCode\nIER_Scores$psychsyn_score &lt;- psychsyn(IER_Sim_Data, critval = .60)\n\n\nSince positive correlations indicate participants are responding consistently (i.e. carefully), multiply these scores by -1 so that higher scores reflect IER.\n\n\nCode\nIER_Scores$psychsyn_Index &lt;- -1 *(IER_Scores$psychsyn_score)\n\n\nNow calculate the psychometric antonym scores for ALL of the personality/behavior variables.\nFirst, I determine how many item pairs have a correlation less than -0.60.\n\n\nCode\npsychant_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychant_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychant_cor$cor &lt; -.60, na.rm = TRUE)\n\n\n[1] 0\n\n\nIf there are enough pairs, proceed with calculating individual scores. There were 0 in this dataset.\nNext, calculate the IRV index for the personality variables.\nThe intra-individual response variability (IRV) is similiar in spirit to the Longstring index. It is defined as the “standard deviation of responses across a set of consecutive item responses for an individual” (Dunn et al. 2018).\nSince all the items are positively worded, I am calculating this over the whole survey and then splitting the survey into four sections.\n\n\nCode\nIER_Scores$irv_scores &lt;- irv(IER_Sim_Data, split = FALSE)\n\n\nCalculate the long string index for the personality variables.For each observation, the length of the maximum uninterrupted string of identical responses is returned.\n\n\nCode\nIER_Scores$careless_long&lt;-longstring(IER_Sim_Data,avg=FALSE)\n\n\nNow I have to clean up all the NA values in the pyschometric synonym and psychometric antonym indices. NA values index indicate that there was no variance within any of the pairs. Since no variance indicates extreme consistency, and these indices are meant to catch inconsistent respondents. I assign a value of -1 to these respondents because they were responding “carefully,” which really means consistently with these indices.\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$psychsyn_Index)\n\nIER_Scores$psychsyn_Index[missing_rows] &lt;- -1\n\n\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$EO_score)\n\nIER_Scores$EO_score[missing_rows] &lt;- -1\n\n\nCompute Mahalanobis D Index\n\n\nCode\nIER_Scores$mahalscore &lt;- mahad(IER_Sim_Data, plot = FALSE, flag = FALSE, confidence = 0.95, na.rm = TRUE)\n\n\nThe below code converts the IER scores to z-scores.\n\n\nCode\nIER_Sim_Data$ZLongstring_Index &lt;- (IER_Scores$careless_long-mean(IER_Scores$careless_long))/sd(IER_Scores$careless_long)\n\nIER_Sim_Data$ZIRVTotal_Index &lt;- (IER_Scores$irv_scores-mean(IER_Scores$irv_scores))/sd(IER_Scores$irv_scores)\n\nIER_Sim_Data$ZEO_score &lt;- (IER_Scores$EO_score-mean(IER_Scores$EO_score))/sd(IER_Scores$EO_score)\n\nIER_Sim_Data$Zpsychsyn_score &lt;- (IER_Scores$psychsyn_Index-mean(na.omit(IER_Scores$psychsyn_Index)))/sd(na.omit(IER_Scores$psychsyn_Index))\n\nIER_Sim_Data$Zmahal &lt;- (IER_Scores$mahalscore-mean(na.omit(IER_Scores$mahalscore)))/sd(na.omit(IER_Scores$mahalscore))\n\n\nAt this point, we have calculated the scores for all of the IER indices and can move forward the the analyses.",
    "crumbs": [
      "Analyses",
      "Mallory"
    ]
  },
  {
    "objectID": "MalloryAnalysis.html#step-one-prepare-the-rstudio-environment",
    "href": "MalloryAnalysis.html#step-one-prepare-the-rstudio-environment",
    "title": "Mallory Analysis",
    "section": "Step One: Prepare the RStudio Environment",
    "text": "Step One: Prepare the RStudio Environment\nThe below code calls the libraries necessary for the technique.\n\n\nCode\nlibrary(haven)\nlibrary(lavaan)\nlibrary(lavaanPlot)",
    "crumbs": [
      "Analyses",
      "Mallory"
    ]
  },
  {
    "objectID": "MalloryAnalysis.html#step-two-model-specification",
    "href": "MalloryAnalysis.html#step-two-model-specification",
    "title": "Mallory Analysis",
    "section": "Step Two: Model Specification",
    "text": "Step Two: Model Specification\nThe lavaan model syntax is used to describe models to be estimated. The syntax defines models using various formula types. The formula types which are used in the technique are below.\nLatent Variable Definition:\nlatent variable =~ indicator1 + indicator2 + indicator3\nVariances and Covariances:\nvariable1 ~~ variable2\nRegression:\nvariable1 ~~ variable2 + variable3 + variable4\nBy default, the lavaan syntax will always fix the factor loading of a latent variable's first indicator to 1. Add the argument std.lv = TRUE to the function call to force this factor loading to be free.\n\nBaseline Model\nFirst, you must establish a baseline model that will be used in subsequent models and test. The baseline model is a confirmatory factor analysis (CFA) model of the substantive variables and IER variables. Estimates of the unstandardized factor loadings and error variances for the IER variables are retained from this model to use as fixed parameters in subsequent models (Williams, 2016). Additionally, the factor correlations between the substantive variables are recorded to be used in a later model.\nThe code to specify the baseline for this example is as follows:\n\n\nCode\nBaselineModel &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\nfri =~ ri1 + ri2 + ri3\nfwom =~ wom1 + wom2 + wom3\nfsat =~ sat1 + sat2 + sat3\nfdj =~ dj1 + dj2 + dj3\nfij =~ ij1 + ij2 + ij3 + ij4\nfpj =~ pj1 + pj2 + pj3 + pj4\n\nfIER1 =~ ZLongstring_Index\nfIER2=~ Zpsychsyn_score\nfIER3=~ Zmahal'\n\nBaselineModel_fit &lt;- cfa(BaselineModel, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(BaselineModel_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n309.303 257.000   0.998   0.020 \n\n\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 1\nThe first model to examine is a structural model without IER effects which allows for correlations among the substantive latent variables with an orthogonal IER variable. The IER variable’s indicators are the standardized IER indices retained in the exploratory factor analysis.\nThe IER variable is assumed to be orthogonal, as IER behavior should not be correlated with the substantive variables.\nThe code to specify the Model 1 for this example is as follows:\n\n\nCode\nModel1 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel1_fit &lt;- cfa(Model1, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model1_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n907.885 278.000   0.972   0.067 \n\n\nRecord the substantive factor correlations estimated in this model to use in subsequent models. This can be done by sifting through the lavaan summary output or by utilizing code to extract the estimates from the summary.\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 2\nModel 2 is a saturated structural model with IER effects. With this model, the impact of the IER latent variable is allowed to be different for each of the indicators of the substantive constructs. This is done so that the subsequent test of the presence of IER can be conducted without making any assumptions about the specific nature of these effects.\n\n\nCode\nModel2 &lt;-'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel2_fit &lt;- cfa(Model2, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model2_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n243.756 209.000   0.998   0.018 \n\n\n\n\nModel 3\nModel 3 is identical to Model 2, except that the substantive factor correlations are constrained to their unstandardized estimates from Model 1.\nModel 3 is a restricted saturated structural model with IER effects; the term restricted indicates that the values of the substantively important parameters have been restricted to be equal to the estimates from Model 1. The examination of Model 3 provides a direct statistical test of the impact of IER effects.\nThe code to specify Model 3 for this example is as follows:\n\n\nCode\nModel3 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        \n        fssr ~~ 0.382*fri + 0.936*fwom + 0.311*fsat + 0.805*fdj + 0.602*fij + 0.461*fpj\n        fri ~~ 0.319*fwom + 0.782*fsat + 0.506*fdj + 0.493*fij + 0.547*fpj\n        fwom ~~ 0.260*fsat + 0.692*fdj + 0.655*fij + 0.416*fpj\n        fsat ~~ 0.478*fdj + 0.575*fij + 0.689*fpj\n        fdj ~~ 0.639*fij + 0.586*fpj\n        fij ~~ 0.597*fpj'\n\nModel3_fit &lt;- cfa(Model3, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model3_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n939.566 230.000   0.968   0.079",
    "crumbs": [
      "Analyses",
      "Mallory"
    ]
  },
  {
    "objectID": "MalloryAnalysis.html#step-three-compare-the-models",
    "href": "MalloryAnalysis.html#step-three-compare-the-models",
    "title": "Mallory Analysis",
    "section": "Step Three: Compare the models",
    "text": "Step Three: Compare the models\nFirst, a comparison of Model 1 and Model 2 tests the hypothesis that that there are no method effects due to IER.\n\n\nCode\nlavTestLRT(Model1_fit, Model2_fit)\n\n\n\n  \n\n\n\nNext, a comparison of Model 2 and Model 3 tests whether the substantive variable correlations are significantly biased by IER effects.\n\n\nCode\nlavTestLRT(Model3_fit, Model2_fit)\n\n\n\n  \n\n\n\nExport IER Scores",
    "crumbs": [
      "Analyses",
      "Mallory"
    ]
  },
  {
    "objectID": "BartAnalysis.html",
    "href": "BartAnalysis.html",
    "title": "The Interplay of IER and CMV in Data",
    "section": "",
    "text": "List of all the libraries to use for the analysis\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(foreign)\nlibrary(careless)\nlibrary(haven)\nlibrary(readxl)\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCall the dataset and add 20% Barts\n\n\nCode\nexample_data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\")) |&gt; \n  add_column(cmv1 = 7, .after = \"wom3\") # add the CMV variable\n\n\nRows: 500 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (24): ssr1, ssr2, ssr3, ri1, ri2, ri3, wom1, wom2, wom3, sat1, sat2, sat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbart &lt;- function(existing_data) {\n  # randomly select about half of the columns to be Bart's random point of zoning out\n  col_num &lt;- length(names(existing_data)) \n  existing_data |&gt; \n    # use row numbers to pivot each respondent into a group\n    mutate (row_num = row_number()) |&gt;\n    pivot_longer(cols = -row_num, names_to = \"question\", values_to = \"response\") |&gt; \n    group_by(row_num) |&gt;\n    # randomly select about half of the columns to be Bart's random point of zoning out\n    group_modify ( ~{\n      about_half &lt;- rnorm(1, col_num/2, col_num/10) |&gt; round()\n      \n      # keep the first half of the columns\n      start &lt;- .x |&gt; slice_head(n = col_num - about_half)\n      \n      # replace the second half of the columns with random responses\n      end &lt;- .x |&gt; slice_tail(n = about_half) |&gt;\n      mutate(response = sample(1:7, about_half, replace = TRUE))\n      \n      # put them back together\n      rbind(start, end) \n    }) |&gt;\n    # put the data back into tabular format\n    pivot_wider(names_from = question, values_from = response) |&gt; ungroup() |&gt; select(-row_num)\n}\n\nIER_Sim_Data &lt;- slice_sample(example_data, n = nrow(example_data) * 0.2) |&gt; \n  bart() |&gt; #generate barts\n  bind_rows(slice_sample(example_data, n = nrow(example_data) * 0.8)) %&gt;%  # combine with 80% of the original data\n  slice_sample(n = nrow(.)) # shuffle the data\n\nIER_Sim_Data\n\n\n\n  \n\n\n\nCalculate the even-odd consistency score for the personality variables.\n\n\nCode\nIER_Scores&lt;- data.frame(matrix(ncol = 0, nrow = 500))\n\nIER_Scores$EO_score &lt;- evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4))\n\n\nWarning in evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4)): Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n\n\nWarning: The number of items specified by 'factors' does not match the number of columns in 'x'. \n Please check if this is what you want.\n\n\nNow calculate the psychometric synonym scores for ALL of the personality/behavior variables.\nFirst, I create a dataset for the variables. Then, I determine how many item pairs have a correlation greater than 0.60.\n\n\nCode\npsychsyn_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychsyn_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychsyn_cor$cor &gt; .60, na.rm = TRUE)\n\n\n[1] 27\n\n\nIf there are enough pairs, proceed with calculating individual scores. (there were 49)\n\n\nCode\nIER_Scores$psychsyn_score &lt;- psychsyn(IER_Sim_Data, critval = .60)\n\n\nSince positive correlations indicate participants are responding consistently (i.e. carefully), multiply these scores by -1 so that higher scores reflect IER.\n\n\nCode\nIER_Scores$psychsyn_Index &lt;- -1 *(IER_Scores$psychsyn_score)\n\n\nNow calculate the psychometric antonym scores for ALL of the personality/behavior variables.\nFirst, I determine how many item pairs have a correlation less than -0.60.\n\n\nCode\npsychant_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychant_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychant_cor$cor &lt; -.60, na.rm = TRUE)\n\n\n[1] 0\n\n\nIf there are enough pairs, proceed with calculating individual scores. There were 0 in this dataset.\nNext, calculate the IRV index for the personality variables.\nThe intra-individual response variability (IRV) is similiar in spirit to the Longstring index. It is defined as the “standard deviation of responses across a set of consecutive item responses for an individual” (Dunn et al. 2018).\nSince all the items are positively worded, I am calculating this over the whole survey and then splitting the survey into four sections.\n\n\nCode\nIER_Scores$irv_scores &lt;- irv(IER_Sim_Data, split = FALSE)\n\n\nCalculate the long string index for the personality variables.For each observation, the length of the maximum uninterrupted string of identical responses is returned.\n\n\nCode\nIER_Scores$careless_long&lt;-longstring(IER_Sim_Data,avg=FALSE)\n\n\nNow I have to clean up all the NA values in the pyschometric synonym and psychometric antonym indices. NA values index indicate that there was no variance within any of the pairs. Since no variance indicates extreme consistency, and these indices are meant to catch inconsistent respondents. I assign a value of -1 to these respondents because they were responding “carefully,” which really means consistently with these indices.\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$psychsyn_Index)\n\nIER_Scores$psychsyn_Index[missing_rows] &lt;- -1\n\n\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$EO_score)\n\nIER_Scores$EO_score[missing_rows] &lt;- -1\n\n\nCompute Mahalanobis D Index\n\n\nCode\nIER_Scores$mahalscore &lt;- mahad(IER_Sim_Data, plot = FALSE, flag = FALSE, confidence = 0.95, na.rm = TRUE)\n\n\nThe below code converts the IER scores to z-scores.\n\n\nCode\nIER_Sim_Data$ZLongstring_Index &lt;- (IER_Scores$careless_long-mean(IER_Scores$careless_long))/sd(IER_Scores$careless_long)\n\nIER_Sim_Data$ZIRVTotal_Index &lt;- (IER_Scores$irv_scores-mean(IER_Scores$irv_scores))/sd(IER_Scores$irv_scores)\n\nIER_Sim_Data$ZEO_score &lt;- (IER_Scores$EO_score-mean(IER_Scores$EO_score))/sd(IER_Scores$EO_score)\n\nIER_Sim_Data$Zpsychsyn_score &lt;- (IER_Scores$psychsyn_Index-mean(na.omit(IER_Scores$psychsyn_Index)))/sd(na.omit(IER_Scores$psychsyn_Index))\n\nIER_Sim_Data$Zmahal &lt;- (IER_Scores$mahalscore-mean(na.omit(IER_Scores$mahalscore)))/sd(na.omit(IER_Scores$mahalscore))\n\n\nAt this point, we have calculated the scores for all of the IER indices and can move forward the the analyses.",
    "crumbs": [
      "Analyses",
      "Bart"
    ]
  },
  {
    "objectID": "BartAnalysis.html#step-one-prepare-the-rstudio-environment",
    "href": "BartAnalysis.html#step-one-prepare-the-rstudio-environment",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step One: Prepare the RStudio Environment",
    "text": "Step One: Prepare the RStudio Environment\nThe below code calls the libraries necessary for the technique.\n\n\nCode\nlibrary(haven)\nlibrary(lavaan)\nlibrary(lavaanPlot)",
    "crumbs": [
      "Analyses",
      "Bart"
    ]
  },
  {
    "objectID": "BartAnalysis.html#step-two-model-specification",
    "href": "BartAnalysis.html#step-two-model-specification",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Two: Model Specification",
    "text": "Step Two: Model Specification\nThe lavaan model syntax is used to describe models to be estimated. The syntax defines models using various formula types. The formula types which are used in the technique are below.\nLatent Variable Definition:\nlatent variable =~ indicator1 + indicator2 + indicator3\nVariances and Covariances:\nvariable1 ~~ variable2\nRegression:\nvariable1 ~~ variable2 + variable3 + variable4\nBy default, the lavaan syntax will always fix the factor loading of a latent variable's first indicator to 1. Add the argument std.lv = TRUE to the function call to force this factor loading to be free.\n\nBaseline Model\nFirst, you must establish a baseline model that will be used in subsequent models and test. The baseline model is a confirmatory factor analysis (CFA) model of the substantive variables and IER variables. Estimates of the unstandardized factor loadings and error variances for the IER variables are retained from this model to use as fixed parameters in subsequent models (Williams, 2016). Additionally, the factor correlations between the substantive variables are recorded to be used in a later model.\nThe code to specify the baseline for this example is as follows:\n\n\nCode\nBaselineModel &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal'\n\nBaselineModel_fit &lt;- cfa(BaselineModel, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(BaselineModel_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n499.580 257.000   0.978   0.043 \n\n\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 1\nThe first model to examine is a structural model without IER effects which allows for correlations among the substantive latent variables with an orthogonal IER variable. The IER variable’s indicators are the standardized IER indices retained in the exploratory factor analysis.\nThe IER variable is assumed to be orthogonal, as IER behavior should not be correlated with the substantive variables.\nThe code to specify the Model 1 for this example is as follows:\n\n\nCode\nModel1 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel1_fit &lt;- cfa(Model1, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model1_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n867.056 278.000   0.947   0.065 \n\n\nRecord the substantive factor correlations estimated in this model to use in subsequent models. This can be done by sifting through the lavaan summary output or by utilizing code to extract the estimates from the summary.\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 2\nModel 2 is a saturated structural model with IER effects. With this model, the impact of the IER latent variable is allowed to be different for each of the indicators of the substantive constructs. This is done so that the subsequent test of the presence of IER can be conducted without making any assumptions about the specific nature of these effects.\n\n\nCode\nModel2 &lt;-'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel2_fit &lt;- cfa(Model2, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model2_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n398.843 209.000   0.983   0.043 \n\n\n\n\nModel 3\nModel 3 is identical to Model 2, except that the substantive factor correlations are constrained to their unstandardized estimates from Model 1.\nModel 3 is a restricted saturated structural model with IER effects; the term restricted indicates that the values of the substantively important parameters have been restricted to be equal to the estimates from Model 1. The examination of Model 3 provides a direct statistical test of the impact of IER effects.\nThe code to specify Model 3 for this example is as follows:\n\n\nCode\nModel3 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        \n        fssr ~~ 0.382*fri + 0.936*fwom + 0.311*fsat + 0.805*fdj + 0.602*fij + 0.461*fpj\n        fri ~~ 0.319*fwom + 0.782*fsat + 0.506*fdj + 0.493*fij + 0.547*fpj\n        fwom ~~ 0.260*fsat + 0.692*fdj + 0.655*fij + 0.416*fpj\n        fsat ~~ 0.478*fdj + 0.575*fij + 0.689*fpj\n        fdj ~~ 0.639*fij + 0.586*fpj\n        fij ~~ 0.597*fpj'\n\nModel3_fit &lt;- cfa(Model3, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model3_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n   chisq       df      cfi    rmsea \n1001.713  230.000    0.931    0.082",
    "crumbs": [
      "Analyses",
      "Bart"
    ]
  },
  {
    "objectID": "BartAnalysis.html#step-three-compare-the-models",
    "href": "BartAnalysis.html#step-three-compare-the-models",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Three: Compare the models",
    "text": "Step Three: Compare the models\nFirst, a comparison of Model 1 and Model 2 tests the hypothesis that that there are no method effects due to IER.\n\n\nCode\nlavTestLRT(Model1_fit, Model2_fit)\n\n\n\n  \n\n\n\nNext, a comparison of Model 2 and Model 3 tests whether the substantive variable correlations are significantly biased by IER effects.\n\n\nCode\nlavTestLRT(Model3_fit, Model2_fit)\n\n\n\n  \n\n\n\nExport IER Scores\n\n\n Download data",
    "crumbs": [
      "Analyses",
      "Bart"
    ]
  }
]