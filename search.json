[
  {
    "objectID": "Avatars.html",
    "href": "Avatars.html",
    "title": "IER Avatars",
    "section": "",
    "text": "To create simulated insufficient error response data, we will use avatars. Each avatar has a name and an IER pattern.",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-1-mallory",
    "href": "Avatars.html#avatar-1-mallory",
    "title": "IER Avatars",
    "section": "Avatar 1: Mallory",
    "text": "Avatar 1: Mallory\nMallory is an accountant at a hotel management company. The HR manager at Mallory’s company is friends with a professor at a local university, so about once every year, Mallory and her coworkers are asked to complete surveys for research studies. As an incentive for participating in a survey when asked, if at least 50% of the company completes the survey, the office closes early the Friday after the survey closes. Most of the surveys ask about work environment and behaviors, including citizenship behaviors, organizational justice, workplace deviance, and job performance. When Mallory completed the first few surveys, she paid close attention to the survey items and answered thoughtfully and honestly, thinking the results of the survey might lead to improvements in the office (and a half-day off work). However, after about five years of taking these surveys, she realized changes weren’t coming, only the half-day off. At that point, her only motivation for participating in the survey was to make sure she did her part to get the survey participation rate up to the goal that was set for the survey. As a result, Mallory has stopped reading the questions in the survey altogether and just selects “neutral” for every item.\n\nComments\nThis one seems pretty simple. It is a straightline responder for the entire survey.",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-2-darren",
    "href": "Avatars.html#avatar-2-darren",
    "title": "IER Avatars",
    "section": "Avatar 2: Darren",
    "text": "Avatar 2: Darren\nDarren is an attorney and works for the same company as Mallory. He’s never been a fan of the surveys because frankly, he feels like he is way too busy for something as silly as academic research. However, everyone hounds him to complete the surveys when they come around because of the promise of a half-day off work if the participation rate for the survey is high enough. He’s worried that if he doesn’t appear to take the survey seriously, his responses won’t be recorded, rendering his time completing the survey completely wasted. So, to appear like he’s thoughtfully responding to the survey, he randomly selects responses to the survey items so that there’s no risk of his responses being scrapped.\n\nComments\nThis one seems pretty simple, too. It is a random responder for the entire survey.",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-3-bart",
    "href": "Avatars.html#avatar-3-bart",
    "title": "IER Avatars",
    "section": "Avatar 3: Bart",
    "text": "Avatar 3: Bart\nBart is a financial analyst who lives and works in a large metropolitan area. He commutes to work by train every day, and the trip is about 45 minutes each way. His work is stressful and draining, so sometimes he completes surveys online to wind down after a long day. He gets paid to do it, but that isn’t even something he thinks about when selecting surveys to complete. Bart’s “payment” is the enjoyment he gets from completing surveys. He likes to be introspective, likening it to meditation, so completing surveys is fascinating to him. So, when Bart completes a survey, he gives it his full attention. Well, he gives it his full attention for as long as he can. Some surveys are just too long, and he gets fatigued by repetitive questions that seem to go on and on. When that happens, Bart tries to give his brain a break by skimming questions or randomly allowing himself to zone out while reading questions after he’s been in a survey for a while.\n\nComments\nThis is the trickier one. Currently, our survey is too short to simulate Bart. Christie and I had talked about needing to add more variables to our simulation for this. However, Christie mentioned that they have always kept the scales short for our CMV studies because CMV is more likely to be detected that way. Are we going to be washing out the CMV possibility with a longer set of items? My question is—how does a longer list of short scales affect our ability to detect CMV? I’d assume in a similar way, right?",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-4-scarlett",
    "href": "Avatars.html#avatar-4-scarlett",
    "title": "IER Avatars",
    "section": "Avatar 4: Scarlett",
    "text": "Avatar 4: Scarlett\nScarlett is an office manager who spends most of her day at work filing paperwork and ordering office supplies. She has quite a bit of downtime at work, so she completes surveys to earn some extra cash. However, she has often had to rush through completing surveys due to being called into meetings last minute. Scarlett has read on blogs and message boards that researchers often use trap questions and consistency screening methods to catch participants who are rushing or careless, and those participants don’t get paid for their work. So, when she must rush through a survey (whether it’s halfway through a survey or for its entirety), she’s come up with her own system. First, she reads the first question on the page to decide how to answer the rest of the items. Since she doesn’t want to seem too consistent, she varies her answers for the rest of the page around her answer to the first item. Then, while answering the items on the rest of the page in a nonrandom pattern, she pays just enough attention to the survey items to spot catch questions such as, “Please mark STRONGLY AGREE for this item,” or “I am paid biweekly by gremlins,” so that she will answer those items correctly.\n\nComments\nFor this avatar, we’d need the response to the first item on a page to be selected randomly. Then, each subsequent response is a random choice within a constrained range around the first response. For example, if “4” is the response for the first item, subsequent items on the page should be a random selection of 3, 4, or 5.",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "CMVSimulation2.html",
    "href": "CMVSimulation2.html",
    "title": "CMV Simulation 2",
    "section": "",
    "text": "This document will go through simulating CMV and IEP."
  },
  {
    "objectID": "CMVSimulation2.html#cmv-simulation-2",
    "href": "CMVSimulation2.html#cmv-simulation-2",
    "title": "CMV Simulation 2",
    "section": "",
    "text": "This document will go through simulating CMV and IEP."
  },
  {
    "objectID": "CMVSimulation2.html#initialization",
    "href": "CMVSimulation2.html#initialization",
    "title": "CMV Simulation 2",
    "section": "Initialization",
    "text": "Initialization\nWe will need the following projects:\n\ntidyverse: Data wrangling\nlavaan: SEM functionality\nsemPlot: Plotting SEM diagrams\n\nWe’ll also set the random seed to an arbitrary fixed value to allow for reproduceability\n\n\nCode\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nset.seed(1337)"
  },
  {
    "objectID": "CMVSimulation2.html#model",
    "href": "CMVSimulation2.html#model",
    "title": "CMV Simulation 2",
    "section": "Model",
    "text": "Model\nOur model has three latent variables that are each measured with several items:\n\nMobile phone addiction: phone_add\n\nMissed Work: mw\nConcentration: conc\nPain: pain\nRequire Phone: reqp\nImpatient: imp\nThinking: think\nNever Give Up: ngu\nConstant Checking: check\nExtended Usage: use\nPerceived Extended Usage: puse\n\nPerceived Stress: stress\n\nUpset: upset\nUnable to Control: nocont\nNervous: nerv\nIrritating Hassles: hass\nCoping: cope\nConfident: conf\nGoing Your Way: way\nNo Cope: nocope\nControl: cont\nOn Top: ontop\nAngered: angr\nAnxious: anx\nControl Time: time\nPiling: pile\n\nPerceived Learning: plearn\n\nOrganize: org\nGuide: guid\nUse Knowledge: usek\nAttitude: att\nCritique: crit\nSelf-Reliant: selfr\nSophisticated: soph\n\n\nWe’ll input the model in lavaan formula syntax and have it create an SEM diagram for us\n\n\nCode\ncfa_model &lt;- '\n  # measurement model\n  phone_add =~ mw + conc + pain + reqp + imp + think + ngu + check + use + puse\n  stress =~ upset + nocont + nerv + hass + cop + conf + way + nocope + cont + ontop + angr + anx + time + pile\n  plearn =~ org + guid + usek + att + crit + selfr + soph\n  \n  # covariances\n  phone_add ~~ stress\n  phone_add ~~ plearn\n  stress ~~ plearn\n'\n\nsemPlotModel_lavaanModel(cfa_model) |&gt; \n  semPaths(layout=\"tree\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), label.norm = \"OOOOO\", residuals = FALSE)"
  },
  {
    "objectID": "CMVSimulation2.html#simulation",
    "href": "CMVSimulation2.html#simulation",
    "title": "CMV Simulation 2",
    "section": "Simulation",
    "text": "Simulation\nNow we’ll simulate data for this model. In this case all item weights are simulated as being equal\n\n\n\n\n\n\nNote\n\n\n\nRight now everything is coded in the same direction. Will need to fix loading at -1 for reverse coded items\n\n\n\n\nCode\ncfa_sim &lt;- simulateData(cfa_model, model.type = \"cfa\", sample.nobs = 500)\n#change to likert scores\ncfa_sim &lt;- cfa_sim |&gt; mutate(across(everything(), \\(x) findInterval(x, vec=c(-Inf,-2.5,-1, 1,2.5,Inf))))\ncfa_sim\n\n\n\n  \n\n\n\nCheck the fit of the simulation\n\n\nCode\nfit &lt;- cfa(cfa_model, cfa_sim, ordered = TRUE)\nsummary(fit)\n\n\nlavaan 0.6-18 ended normally after 43 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                       158\n\n  Number of observations                           500\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               347.483     465.802\n  Degrees of freedom                               431         431\n  P-value (Chi-square)                           0.999       0.120\n  Scaling correction factor                                  1.329\n  Shift parameter                                          204.263\n    simple second-order correction                                \n\nParameter Estimates:\n\n  Parameterization                               Delta\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  phone_add =~                                        \n    mw                1.000                           \n    conc              1.012    0.045   22.424    0.000\n    pain              0.980    0.046   21.419    0.000\n    reqp              0.977    0.046   21.256    0.000\n    imp               0.999    0.045   22.125    0.000\n    think             0.957    0.047   20.349    0.000\n    ngu               0.949    0.050   19.164    0.000\n    check             0.922    0.047   19.455    0.000\n    use               0.989    0.046   21.612    0.000\n    puse              1.026    0.048   21.494    0.000\n  stress =~                                           \n    upset             1.000                           \n    nocont            1.000    0.043   23.132    0.000\n    nerv              1.055    0.046   22.838    0.000\n    hass              1.048    0.042   25.067    0.000\n    cop               0.991    0.048   20.530    0.000\n    conf              0.989    0.047   20.915    0.000\n    way               1.070    0.046   23.247    0.000\n    nocope            1.020    0.044   23.027    0.000\n    cont              0.960    0.045   21.191    0.000\n    ontop             1.005    0.044   23.094    0.000\n    angr              0.992    0.046   21.478    0.000\n    anx               1.007    0.044   22.784    0.000\n    time              0.905    0.049   18.556    0.000\n    pile              0.996    0.045   22.191    0.000\n  plearn =~                                           \n    org               1.000                           \n    guid              1.055    0.054   19.475    0.000\n    usek              0.972    0.053   18.495    0.000\n    att               1.014    0.057   17.890    0.000\n    crit              1.015    0.057   17.867    0.000\n    selfr             1.018    0.055   18.444    0.000\n    soph              1.083    0.054   20.086    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  phone_add ~~                                        \n    stress            0.005    0.025    0.196    0.845\n    plearn           -0.013    0.025   -0.533    0.594\n  stress ~~                                           \n    plearn            0.031    0.024    1.284    0.199\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    mw|t1            -1.825    0.108  -16.973    0.000\n    mw|t2            -0.668    0.061  -10.971    0.000\n    mw|t3             0.739    0.062   11.909    0.000\n    mw|t4             1.607    0.092   17.416    0.000\n    conc|t1          -1.706    0.099  -17.297    0.000\n    conc|t2          -0.687    0.061  -11.228    0.000\n    conc|t3           0.745    0.062   11.993    0.000\n    conc|t4           1.774    0.104   17.134    0.000\n    pain|t1          -1.977    0.121  -16.300    0.000\n    pain|t2          -0.656    0.061  -10.800    0.000\n    pain|t3           0.745    0.062   11.993    0.000\n    pain|t4           1.799    0.105   17.059    0.000\n    reqp|t1          -1.728    0.100  -17.252    0.000\n    reqp|t2          -0.706    0.062  -11.484    0.000\n    reqp|t3           0.700    0.061   11.399    0.000\n    reqp|t4           1.774    0.104   17.134    0.000\n    imp|t1           -1.728    0.100  -17.252    0.000\n    imp|t2           -0.650    0.061  -10.714    0.000\n    imp|t3            0.706    0.062   11.484    0.000\n    imp|t4            1.774    0.104   17.134    0.000\n    think|t1         -1.825    0.108  -16.973    0.000\n    think|t2         -0.693    0.061  -11.314    0.000\n    think|t3          0.706    0.062   11.484    0.000\n    think|t4          1.825    0.108   16.973    0.000\n    ngu|t1           -1.706    0.099  -17.297    0.000\n    ngu|t2           -0.687    0.061  -11.228    0.000\n    ngu|t3            0.732    0.062   11.824    0.000\n    ngu|t4            1.852    0.110   16.873    0.000\n    check|t1         -1.645    0.095  -17.388    0.000\n    check|t2         -0.662    0.061  -10.886    0.000\n    check|t3          0.793    0.063   12.580    0.000\n    check|t4          1.852    0.110   16.873    0.000\n    use|t1           -1.852    0.110  -16.873    0.000\n    use|t2           -0.650    0.061  -10.714    0.000\n    use|t3            0.726    0.062   11.739    0.000\n    use|t4            1.799    0.105   17.059    0.000\n    puse|t1          -1.665    0.096  -17.364    0.000\n    puse|t2          -0.687    0.061  -11.228    0.000\n    puse|t3           0.772    0.063   12.330    0.000\n    puse|t4           1.943    0.118   16.474    0.000\n    upset|t1         -1.706    0.099  -17.297    0.000\n    upset|t2         -0.713    0.062  -11.570    0.000\n    upset|t3          0.674    0.061   11.057    0.000\n    upset|t4          1.751    0.102   17.198    0.000\n    nocont|t1        -1.728    0.100  -17.252    0.000\n    nocont|t2        -0.637    0.060  -10.541    0.000\n    nocont|t3         0.687    0.061   11.228    0.000\n    nocont|t4         1.943    0.118   16.474    0.000\n    nerv|t1          -1.685    0.097  -17.334    0.000\n    nerv|t2          -0.739    0.062  -11.909    0.000\n    nerv|t3           0.719    0.062   11.655    0.000\n    nerv|t4           1.852    0.110   16.873    0.000\n    hass|t1          -1.751    0.102  -17.198    0.000\n    hass|t2          -0.765    0.063  -12.246    0.000\n    hass|t3           0.662    0.061   10.886    0.000\n    hass|t4           1.751    0.102   17.198    0.000\n    cop|t1           -1.626    0.093  -17.405    0.000\n    cop|t2           -0.662    0.061  -10.886    0.000\n    cop|t3            0.706    0.062   11.484    0.000\n    cop|t4            1.665    0.096   17.364    0.000\n    conf|t1          -1.555    0.089  -17.421    0.000\n    conf|t2          -0.668    0.061  -10.971    0.000\n    conf|t3           0.693    0.061   11.314    0.000\n    conf|t4           1.825    0.108   16.973    0.000\n    way|t1           -1.751    0.102  -17.198    0.000\n    way|t2           -0.713    0.062  -11.570    0.000\n    way|t3            0.745    0.062   11.993    0.000\n    way|t4            1.685    0.097   17.334    0.000\n    nocope|t1        -1.911    0.115  -16.626    0.000\n    nocope|t2        -0.779    0.063  -12.413    0.000\n    nocope|t3         0.759    0.062   12.162    0.000\n    nocope|t4         1.665    0.096   17.364    0.000\n    cont|t1          -1.706    0.099  -17.297    0.000\n    cont|t2          -0.719    0.062  -11.655    0.000\n    cont|t3           0.687    0.061   11.228    0.000\n    cont|t4           1.665    0.096   17.364    0.000\n    ontop|t1         -1.607    0.092  -17.416    0.000\n    ontop|t2         -0.619    0.060  -10.282    0.000\n    ontop|t3          0.668    0.061   10.971    0.000\n    ontop|t4          1.685    0.097   17.334    0.000\n    angr|t1          -1.706    0.099  -17.297    0.000\n    angr|t2          -0.687    0.061  -11.228    0.000\n    angr|t3           0.765    0.063   12.246    0.000\n    angr|t4           1.825    0.108   16.973    0.000\n    anx|t1           -1.589    0.091  -17.423    0.000\n    anx|t2           -0.625    0.060  -10.368    0.000\n    anx|t3            0.732    0.062   11.824    0.000\n    anx|t4            1.706    0.099   17.297    0.000\n    time|t1          -1.665    0.096  -17.364    0.000\n    time|t2          -0.681    0.061  -11.143    0.000\n    time|t3           0.625    0.060   10.368    0.000\n    time|t4           1.665    0.096   17.364    0.000\n    pile|t1          -1.825    0.108  -16.973    0.000\n    pile|t2          -0.732    0.062  -11.824    0.000\n    pile|t3           0.726    0.062   11.739    0.000\n    pile|t4           1.706    0.099   17.297    0.000\n    org|t1           -1.751    0.102  -17.198    0.000\n    org|t2           -0.700    0.061  -11.399    0.000\n    org|t3            0.662    0.061   10.886    0.000\n    org|t4            1.728    0.100   17.252    0.000\n    guid|t1          -1.706    0.099  -17.297    0.000\n    guid|t2          -0.719    0.062  -11.655    0.000\n    guid|t3           0.719    0.062   11.655    0.000\n    guid|t4           1.774    0.104   17.134    0.000\n    usek|t1          -1.728    0.100  -17.252    0.000\n    usek|t2          -0.700    0.061  -11.399    0.000\n    usek|t3           0.739    0.062   11.909    0.000\n    usek|t4           1.881    0.112   16.758    0.000\n    att|t1           -1.943    0.118  -16.474    0.000\n    att|t2           -0.779    0.063  -12.413    0.000\n    att|t3            0.631    0.060   10.455    0.000\n    att|t4            1.607    0.092   17.416    0.000\n    crit|t1          -1.685    0.097  -17.334    0.000\n    crit|t2          -0.687    0.061  -11.228    0.000\n    crit|t3           0.706    0.062   11.484    0.000\n    crit|t4           1.685    0.097   17.334    0.000\n    selfr|t1         -1.881    0.112  -16.758    0.000\n    selfr|t2         -0.813    0.063  -12.829    0.000\n    selfr|t3          0.662    0.061   10.886    0.000\n    selfr|t4          1.589    0.091   17.423    0.000\n    soph|t1          -1.645    0.095  -17.388    0.000\n    soph|t2          -0.713    0.062  -11.570    0.000\n    soph|t3           0.619    0.060   10.282    0.000\n    soph|t4           1.728    0.100   17.252    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .mw                0.467                           \n   .conc              0.453                           \n   .pain              0.488                           \n   .reqp              0.491                           \n   .imp               0.467                           \n   .think             0.512                           \n   .ngu               0.519                           \n   .check             0.546                           \n   .use               0.478                           \n   .puse              0.439                           \n   .upset             0.483                           \n   .nocont            0.483                           \n   .nerv              0.424                           \n   .hass              0.432                           \n   .cop               0.493                           \n   .conf              0.494                           \n   .way               0.408                           \n   .nocope            0.462                           \n   .cont              0.523                           \n   .ontop             0.478                           \n   .angr              0.491                           \n   .anx               0.476                           \n   .time              0.576                           \n   .pile              0.488                           \n   .org               0.510                           \n   .guid              0.454                           \n   .usek              0.537                           \n   .att               0.496                           \n   .crit              0.495                           \n   .selfr             0.493                           \n   .soph              0.426                           \n    phone_add         0.533    0.038   14.018    0.000\n    stress            0.517    0.037   14.144    0.000\n    plearn            0.490    0.041   11.905    0.000"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CMV-IER Simulation Main",
    "section": "",
    "text": "This is a set of pages that run through simulating and testing data for the CMV IER project by Elizabeth Raglund, Christie Fuller, Marcia Simmering, and Doug Twitchell.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#cmv-ier-simulation",
    "href": "index.html#cmv-ier-simulation",
    "title": "CMV-IER Simulation Main",
    "section": "",
    "text": "This is a set of pages that run through simulating and testing data for the CMV IER project by Elizabeth Raglund, Christie Fuller, Marcia Simmering, and Doug Twitchell.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#generate-baseline-data",
    "href": "index.html#generate-baseline-data",
    "title": "CMV-IER Simulation Main",
    "section": "Generate Baseline Data",
    "text": "Generate Baseline Data\nIn this simulation, we generate a data set with no CMV or IER. This is the baseline data set that we will use to compare to the other data sets.\nGenerate Baseline Data",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#investigate-ier-and-cmv-interplay",
    "href": "index.html#investigate-ier-and-cmv-interplay",
    "title": "CMV-IER Simulation Main",
    "section": "Investigate IER and CMV Interplay",
    "text": "Investigate IER and CMV Interplay\nIER CMV Interplay",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#generate-data-from-ier-avatars",
    "href": "index.html#generate-data-from-ier-avatars",
    "title": "CMV-IER Simulation Main",
    "section": "Generate Data from IER Avatars",
    "text": "Generate Data from IER Avatars\nGenerate Data from IER Avatars",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "SimpleGriddedCMVSimulation.html",
    "href": "SimpleGriddedCMVSimulation.html",
    "title": "Simple Gridded CVM Simulation",
    "section": "",
    "text": "In this document, we’ll do a simple CMV Simulation to show that our technique works. We’ll then use the same technique on our real data. We will also show how to do a gridded simulation with a range of CMV and other factor loadings."
  },
  {
    "objectID": "SimpleGriddedCMVSimulation.html#simple-cmv-simulation",
    "href": "SimpleGriddedCMVSimulation.html#simple-cmv-simulation",
    "title": "Simple Gridded CVM Simulation",
    "section": "",
    "text": "In this document, we’ll do a simple CMV Simulation to show that our technique works. We’ll then use the same technique on our real data. We will also show how to do a gridded simulation with a range of CMV and other factor loadings."
  },
  {
    "objectID": "SimpleGriddedCMVSimulation.html#initialization",
    "href": "SimpleGriddedCMVSimulation.html#initialization",
    "title": "Simple Gridded CVM Simulation",
    "section": "Initialization",
    "text": "Initialization\nWe will need the following projects:\n\ntidyverse: Data wrangling\nlavaan: SEM functionality\nsimstandard: Does lavaan simulation properly (lavaan’s SimulateData() has problems) and provides some handy functions for working with SEM data in data frames\nsemPlot: Plotting SEM diagrams\n\nWe’ll also set the random seed to an arbitrary fixed value to allow for reproduceability\nFinally, we need several scripts in cmv_sim_functions.R\n\n\nCode\nlibrary(tidyverse)\nlibrary(furrr)\nlibrary(lavaan)\nlibrary(simstandard)\nlibrary(semPlot)\n\nset.seed(1337)\nplan(\"multicore\") # when using parallel processing in furrr, use all cores\n\nsource(\"cmv_sim_functions.R\", local = knitr::knit_global())"
  },
  {
    "objectID": "SimpleGriddedCMVSimulation.html#model",
    "href": "SimpleGriddedCMVSimulation.html#model",
    "title": "Simple Gridded CVM Simulation",
    "section": "Model",
    "text": "Model\nOur model has three latent variables that are each measured with two or three items. Additionally, each item is measured using a 5-likert scale.\n\nX\n\nx1\nx2\nx3\n\nY\n\ny1\ny2\ny3\n\nZ\n\nz1\nz2\nz3\n\n\nWe’ll input the model in lavaan formula syntax and have it create an SEM diagram for us\n\n\nCode\ncfa_model &lt;- '\n  # measurement model\n  X =~ x1 + x2 + x3\n  Y =~ y1 + y2 + y3\n  Z =~ z1 + z2 + z3\n  \n  # covariances\n  X ~~ Y\n  Y ~~ Z\n  X ~~ Z\n'\n\nsemPlotModel_lavaanModel(cfa_model) |&gt; \n  semPaths(layout=\"tree\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), label.norm = \"OOOOO\", residuals = FALSE)"
  },
  {
    "objectID": "SimpleGriddedCMVSimulation.html#simulation",
    "href": "SimpleGriddedCMVSimulation.html#simulation",
    "title": "Simple Gridded CVM Simulation",
    "section": "Simulation",
    "text": "Simulation\nTo simulate, we need to set the parameter values. We’ll do this for each kind of parameter. Before we do that, though, we need to convert the model to a lavaan parameter table. We will also add the CMV marker variable (using a function in cmv_sim_functions.R)\n\n\nCode\ncfa_model_pars &lt;- lavaanify(cfa_model, std.lv = TRUE) |&gt; add_cmv_marker()\n\nsemPlotModel_lavaanModel(cfa_model_pars) |&gt; \n  semPaths(layout=\"tree\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), label.norm = \"OOOOO\", residuals = FALSE)\n\n\n\n\n\n\n\n\n\n\nparameters\nWe’ll create a parameter grid varying the CMV loadings from 0 to 1 and keeping the factor loadings constant at 0.8. We’ll put in a little bit of between factor covariance, but he covariance between the factors and the marker is set to 0.\n\n\nCode\nloadings &lt;- 0.9\ncmv_loadings = seq(0.1, 0.2, by = 0.1)\ncovariances &lt;- 0 \ncfa_model_pars &lt;- cfa_model_pars |&gt; fix_cmv_loadings(loadings, invert = TRUE) |&gt; fix_covariances(covariances)\ncfa_model_pars_list &lt;- map(cmv_loadings, \\(x) fix_cmv_loadings(cfa_model_pars, x)) # make a list of data frames with each frame having one of the CMV loadings\nfits &lt;- map(cfa_model_pars_list, \\(x) { #TODO: change to future_map\n    cfa_sim &lt;- sim_standardized(x, latent = FALSE, errors = FALSE, factor_scores = FALSE,  composites = FALSE, matrices = FALSE) |&gt; \n    # create likert scores using standard z scores for quintiles\n    mutate(across(everything(), \\(y) findInterval(y, vec=c(-Inf, -1.2,-0.4, 0.4, 1.2,Inf)))) \n    print(tibble(cfa_sim))\n    return(cfa(fixed2free(cfa_model_pars), cfa_sim))\n}) \n\n\nWarning: lavaan-&gt;lavParTable():  \n   input already looks like a parameter table\n\n\nWarning in sim_standardized_matrices(m, ...): Because the following relationship was not set, it is assumed to be 0:\nX =~ x1\n\n\nWarning: lavaan-&gt;lavaanify():  \n   input already looks like a parameter table\n\n\n# A tibble: 1,000 × 16\n      x1    x2    x3    y1    y2    y3    z1    z2    z3  CMV1  CMV2  CMV3  CMV4\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     4     3     3     5     3     5     5     5     5     2     4     1     3\n 2     3     2     2     3     3     5     2     2     1     3     5     4     2\n 3     2     1     1     4     3     4     4     4     4     2     3     4     2\n 4     1     2     3     3     4     3     5     5     4     1     4     3     2\n 5     4     4     5     3     4     3     2     3     3     2     5     1     3\n 6     2     2     3     4     3     3     2     3     3     2     3     2     2\n 7     4     5     4     4     5     5     5     5     5     1     2     3     3\n 8     3     2     3     1     1     1     2     3     2     5     1     2     4\n 9     4     3     3     5     5     4     3     2     3     5     5     2     4\n10     4     4     4     2     3     3     3     3     2     4     3     4     2\n# ℹ 990 more rows\n# ℹ 3 more variables: CMV5 &lt;int&gt;, CMV6 &lt;int&gt;, CMV7 &lt;int&gt;\n\n\nWarning: lavaan-&gt;lavaanify():  \n   input already looks like a parameter table\n\n\nWarning: lavaan-&gt;lav_lavaan_step11_estoptim():  \n   Model estimation FAILED! Returning starting values.\n\n\nWarning: lavaan-&gt;lavParTable():  \n   input already looks like a parameter table\n\n\nWarning in sim_standardized_matrices(m, ...): Because the following relationship was not set, it is assumed to be 0:\nX =~ x1\n\n\nWarning: lavaan-&gt;lavaanify():  \n   input already looks like a parameter table\n\n\n# A tibble: 1,000 × 16\n      x1    x2    x3    y1    y2    y3    z1    z2    z3  CMV1  CMV2  CMV3  CMV4\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     3     3     2     1     1     1     1     1     1     2     4     1     1\n 2     1     1     1     2     2     2     1     2     2     2     4     3     3\n 3     3     2     3     3     3     4     2     1     1     3     3     3     5\n 4     1     1     1     4     4     4     1     1     1     2     4     3     5\n 5     3     3     4     2     3     2     2     2     1     3     3     3     4\n 6     3     4     4     1     2     1     5     5     5     4     5     3     4\n 7     3     3     2     3     4     3     4     5     5     2     4     2     5\n 8     1     1     1     1     2     2     2     2     1     5     1     4     4\n 9     1     1     1     3     3     4     2     3     3     3     2     4     3\n10     3     2     3     3     5     5     4     4     4     4     3     1     4\n# ℹ 990 more rows\n# ℹ 3 more variables: CMV5 &lt;int&gt;, CMV6 &lt;int&gt;, CMV7 &lt;int&gt;\n\n\nWarning: lavaan-&gt;lavaanify():  \n   input already looks like a parameter table\n\n\nWarning: lavaan-&gt;lav_model_vcov():  \n   Could not compute standard errors! The information matrix could not be \n   inverted. This may be a symptom that the model is not identified.\n\n\nCode\n#test &lt;- do.call(lavTestLRT, fits)\n#tibble(test)\n\n#fit_measures &lt;- map(fits, \\(x) fitmeasures(x))  |&gt;\n  # convert from list of measures to tibble\n#  map_dfr(~tibble(fit_measure = names(.x), value = .x), .id = \"run\")"
  },
  {
    "objectID": "IER-CMVInterplay.html",
    "href": "IER-CMVInterplay.html",
    "title": "The Interplay of IER and CMV in Data",
    "section": "",
    "text": "List of all the libraries to use for the analysis\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(foreign)\nlibrary(careless)\nlibrary(haven)\nlibrary(readxl)\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCall the dataset\n\n\nCode\nIER_Sim_Data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))\n\n\nRows: 500 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (24): ssr1, ssr2, ssr3, ri1, ri2, ri3, wom1, wom2, wom3, sat1, sat2, sat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCalculate the even-odd consistency score for the personality variables.\n\n\nCode\nIER_Scores&lt;- data.frame(matrix(ncol = 0, nrow = 500))\n\nIER_Scores$EO_score &lt;- evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4))\n\n\nWarning in evenodd(IER_Sim_Data, c(3, 3, 3, 3, 4, 4, 4)): Computation of even-odd has changed for consistency of interpretation\n          with other indices. This change occurred in version 1.2.0. A higher\n          score now indicates a greater likelihood of careless responding. If\n          you have previously written code to cut score based on the output of\n          this function, you should revise that code accordingly.\n\n\nNow calculate the psychometric synonym scores for ALL of the personality/behavior variables.\nFirst, I create a dataset for the variables. Then, I determine how many item pairs have a correlation greater than 0.60.\n\n\nCode\npsychsyn_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychsyn_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychsyn_cor$cor &gt; .60, na.rm = TRUE)\n\n\n[1] 55\n\n\nIf there are enough pairs, proceed with calculating individual scores. (there were 49)\n\n\nCode\nIER_Scores$psychsyn_score &lt;- psychsyn(IER_Sim_Data, critval = .60)\n\n\nSince positive correlations indicate participants are responding consistently (i.e. carefully), multiply these scores by -1 so that higher scores reflect IER.\n\n\nCode\nIER_Scores$psychsyn_Index &lt;- -1 *(IER_Scores$psychsyn_score)\n\n\nNow calculate the psychometric antonym scores for ALL of the personality/behavior variables.\nFirst, I determine how many item pairs have a correlation less than -0.60.\n\n\nCode\npsychant_cor &lt;- psychsyn_critval(IER_Sim_Data)\nhead(psychant_cor)\n\n\n\n  \n\n\n\nCode\nsum(psychant_cor$cor &lt; -.60, na.rm = TRUE)\n\n\n[1] 0\n\n\nIf there are enough pairs, proceed with calculating individual scores. There were 0 in this dataset.\nNext, calculate the IRV index for the personality variables.\nThe intra-individual response variability (IRV) is similiar in spirit to the Longstring index. It is defined as the “standard deviation of responses across a set of consecutive item responses for an individual” (Dunn et al. 2018).\nSince all the items are positively worded, I am calculating this over the whole survey and then splitting the survey into four sections.\n\n\nCode\nIER_Scores$irv_scores &lt;- irv(IER_Sim_Data, split = FALSE)\n\n\nCalculate the long string index for the personality variables.For each observation, the length of the maximum uninterrupted string of identical responses is returned.\n\n\nCode\nIER_Scores$careless_long&lt;-longstring(IER_Sim_Data,avg=FALSE)\n\n\nNow I have to clean up all the NA values in the pyschometric synonym and psychometric antonym indices. NA values index indicate that there was no variance within any of the pairs. Since no variance indicates extreme consistency, and these indices are meant to catch inconsistent respondents. I assign a value of -1 to these respondents because they were responding “carefully,” which really means consistently with these indices.\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$psychsyn_Index)\n\nIER_Scores$psychsyn_Index[missing_rows] &lt;- -1\n\n\n\n\nCode\nmissing_rows &lt;- !complete.cases(IER_Scores$EO_score)\n\nIER_Scores$EO_score[missing_rows] &lt;- -1\n\n\nCompute Mahalanobis D Index\n\n\nCode\nIER_Scores$mahalscore &lt;- mahad(IER_Sim_Data, plot = FALSE, flag = FALSE, confidence = 0.95, na.rm = TRUE)\n\n\nThe below code converts the IER scores to z-scores.\n\n\nCode\nIER_Sim_Data$ZLongstring_Index &lt;- (IER_Scores$careless_long-mean(IER_Scores$careless_long))/sd(IER_Scores$careless_long)\n\nIER_Sim_Data$ZIRVTotal_Index &lt;- (IER_Scores$irv_scores-mean(IER_Scores$irv_scores))/sd(IER_Scores$irv_scores)\n\nIER_Sim_Data$ZEO_score &lt;- (IER_Scores$EO_score-mean(IER_Scores$EO_score))/sd(IER_Scores$EO_score)\n\nIER_Sim_Data$Zpsychsyn_score &lt;- (IER_Scores$psychsyn_Index-mean(na.omit(IER_Scores$psychsyn_Index)))/sd(na.omit(IER_Scores$psychsyn_Index))\n\nIER_Sim_Data$Zmahal &lt;- (IER_Scores$mahalscore-mean(na.omit(IER_Scores$mahalscore)))/sd(na.omit(IER_Scores$mahalscore))\n\n\nAt this point, we have calculated the scores for all of the IER indices and can move forward the the analyses.",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "IER-CMVInterplay.html#step-one-prepare-the-rstudio-environment",
    "href": "IER-CMVInterplay.html#step-one-prepare-the-rstudio-environment",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step One: Prepare the RStudio Environment",
    "text": "Step One: Prepare the RStudio Environment\nThe below code calls the libraries necessary for the technique.\n\n\nCode\nlibrary(haven)\nlibrary(lavaan)\nlibrary(lavaanPlot)",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "IER-CMVInterplay.html#step-two-model-specification",
    "href": "IER-CMVInterplay.html#step-two-model-specification",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Two: Model Specification",
    "text": "Step Two: Model Specification\nThe lavaan model syntax is used to describe models to be estimated. The syntax defines models using various formula types. The formula types which are used in the technique are below.\nLatent Variable Definition:\nlatent variable =~ indicator1 + indicator2 + indicator3\nVariances and Covariances:\nvariable1 ~~ variable2\nRegression:\nvariable1 ~~ variable2 + variable3 + variable4\nBy default, the lavaan syntax will always fix the factor loading of a latent variable's first indicator to 1. Add the argument std.lv = TRUE to the function call to force this factor loading to be free.\n\nBaseline Model\nFirst, you must establish a baseline model that will be used in subsequent models and test. The baseline model is a confirmatory factor analysis (CFA) model of the substantive variables and IER variables. Estimates of the unstandardized factor loadings and error variances for the IER variables are retained from this model to use as fixed parameters in subsequent models (Williams, 2016). Additionally, the factor correlations between the substantive variables are recorded to be used in a later model.\nThe code to specify the baseline for this example is as follows:\n\n\nCode\nBaselineModel &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal'\n\nBaselineModel_fit &lt;- cfa(BaselineModel, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(BaselineModel_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n232.488 257.000   1.000   0.000 \n\n\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 1\nThe first model to examine is a structural model without IER effects which allows for correlations among the substantive latent variables with an orthogonal IER variable. The IER variable’s indicators are the standardized IER indices retained in the exploratory factor analysis.\nThe IER variable is assumed to be orthogonal, as IER behavior should not be correlated with the substantive variables.\nThe code to specify the Model 1 for this example is as follows:\n\n\nCode\nModel1 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index\n        fIER2=~ Zpsychsyn_score\n        fIER3=~ Zmahal\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel1_fit &lt;- cfa(Model1, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model1_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n662.195 278.000   0.982   0.053 \n\n\nRecord the substantive factor correlations estimated in this model to use in subsequent models. This can be done by sifting through the lavaan summary output or by utilizing code to extract the estimates from the summary.\nThe summary function calls a summary of the fitted model. The factor loadings are found in the “Latent Variables” section of the summary output, and the unstandardized error variances are found in the “Variances” section of the output. The code to view the summary output is as follows:\n\n\nModel 2\nModel 2 is a saturated structural model with IER effects. With this model, the impact of the IER latent variable is allowed to be different for each of the indicators of the substantive constructs. This is done so that the subsequent test of the presence of IER can be conducted without making any assumptions about the specific nature of these effects.\n\n\nCode\nModel2 &lt;-'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj'\n\nModel2_fit &lt;- cfa(Model2, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model2_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n180.373 209.000   1.000   0.000 \n\n\n\n\nModel 3\nModel 3 is identical to Model 2, except that the substantive factor correlations are constrained to their unstandardized estimates from Model 1.\nModel 3 is a restricted saturated structural model with IER effects; the term restricted indicates that the values of the substantively important parameters have been restricted to be equal to the estimates from Model 1. The examination of Model 3 provides a direct statistical test of the impact of IER effects.\nThe code to specify Model 3 for this example is as follows:\n\n\nCode\nModel3 &lt;- 'fssr =~ ssr1 + ssr2 + ssr3\n        fri =~ ri1 + ri2 + ri3\n        fwom =~ wom1 + wom2 + wom3\n        fsat =~ sat1 + sat2 + sat3\n        fdj =~ dj1 + dj2 + dj3\n        fij =~ ij1 + ij2 + ij3 + ij4\n        fpj =~ pj1 + pj2 + pj3 + pj4\n\n        fIER1 =~ ZLongstring_Index + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER2=~ Zpsychsyn_score + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        fIER3=~ Zmahal + ssr1 + ssr2 + ssr3 + ri1 + ri2 + ri3 + wom1 + wom2 + wom3 + sat1 + sat2 + sat3 + dj1 + dj2 + dj3 + ij1 + ij2 + ij3 + ij4 + pj1 + pj2 + pj3 + pj4\n        \n        ZLongstring_Index ~~ 0*ZLongstring_Index\n        Zpsychsyn_score ~~ 0*Zpsychsyn_score\n        Zmahal ~~ 0*Zmahal\n        \n        fIER1 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER2 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        fIER3 ~~ 0*fssr + 0*fri + 0*fwom + 0*fsat + 0*fdj + 0*fij + 0*fpj\n        \n        fssr ~~ 0.382*fri + 0.936*fwom + 0.311*fsat + 0.805*fdj + 0.602*fij + 0.461*fpj\n        fri ~~ 0.319*fwom + 0.782*fsat + 0.506*fdj + 0.493*fij + 0.547*fpj\n        fwom ~~ 0.260*fsat + 0.692*fdj + 0.655*fij + 0.416*fpj\n        fsat ~~ 0.478*fdj + 0.575*fij + 0.689*fpj\n        fdj ~~ 0.639*fij + 0.586*fpj\n        fij ~~ 0.597*fpj'\n\nModel3_fit &lt;- cfa(Model3, data=IER_Sim_Data, std.lv=TRUE)\nfitMeasures(Model3_fit, c(\"chisq\", \"df\", \"cfi\", \"rmsea\"))\n\n\n  chisq      df     cfi   rmsea \n808.536 230.000   0.973   0.071",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "IER-CMVInterplay.html#step-three-compare-the-models",
    "href": "IER-CMVInterplay.html#step-three-compare-the-models",
    "title": "The Interplay of IER and CMV in Data",
    "section": "Step Three: Compare the models",
    "text": "Step Three: Compare the models\nFirst, a comparison of Model 1 and Model 2 tests the hypothesis that that there are no method effects due to IER.\n\n\nCode\nlavTestLRT(Model1_fit, Model2_fit)\n\n\n\n  \n\n\n\nNext, a comparison of Model 2 and Model 3 tests whether the substantive variable correlations are significantly biased by IER effects.\n\n\nCode\nlavTestLRT(Model3_fit, Model2_fit)\n\n\n\n  \n\n\n\nExport IER Scores\n\n\n Download data",
    "crumbs": [
      "IER-CMV Interplay"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html",
    "href": "GenerateBaselineData.html",
    "title": "GenerateBaselineData",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThis is lavaan 0.6-18\nlavaan is FREE software! Please report any bugs.",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#introduction",
    "href": "GenerateBaselineData.html#introduction",
    "title": "GenerateBaselineData",
    "section": "Introduction",
    "text": "Introduction\nIn this document we generate a random dataset using the model first introduced in (Maxham and Netemeyer 2003) and used for a CMV simulation in (Fuller et al. 2016)",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#the-model",
    "href": "GenerateBaselineData.html#the-model",
    "title": "GenerateBaselineData",
    "section": "The model",
    "text": "The model\nThe model we will use has the constructs in Table 1 below\n\n\n\nConstruct\nAbbreviation\nNumber of measures\n\n\n\n\nSatisfaction with Recovery\nSSR\n3\n\n\nReturn Intent1\nRI\n3\n\n\nWord of Mouth\nWOM\n3\n\n\nSatisfaction\nSAT\n3\n\n\nDistributive Justice\nDJ\n4\n\n\nInteractional Justice\nIJ\n4\n\n\nProcedural Justice\nPJ\n4\n\n\n\nEach of these constructs has several measures, and the model is expressed as a confirmatory factor analysis in the lavaan equation below and associated diagram\n\n\nCode\ncfa_sim_model &lt;- \"\n  # Measurement model\n  SSR =~ ssr1 + ssr2 + ssr3\n  RI  =~ ri1 + ri2 + ri3\n  WOM =~ wom1 + wom2 + wom3\n  SAT =~ sat1 + sat2 + sat3\n  DJ  =~ dj1 + dj2 + dj3 + dj4\n  IJ  =~ ij1 + ij2 + ij3 + ij4\n  PJ  =~ pj1 + pj2 + pj3 + pj4\n\n  # Variances\n  SSR ~~ SSR\n  RI ~~ RI\n  WOM ~~ WOM\n  SAT ~~ SAT\n  DJ ~~ DJ\n  IJ ~~ IJ\n  PJ ~~ PJ\n  \n  # Covariances\n  SSR ~~ RI  \n  SSR ~~ WOM \n  SSR ~~ SAT \n  SSR ~~ DJ  \n  SSR ~~ IJ  \n  SSR ~~ PJ  \n  RI ~~ WOM \n  RI ~~ SAT \n  RI ~~ DJ  \n  RI ~~ IJ  \n  RI ~~ PJ  \n  WOM ~~ SAT \n  WOM ~~ DJ  \n  WOM ~~ IJ  \n  WOM ~~ PJ  \n  SAT ~~ DJ  \n  SAT ~~ IJ  \n  SAT ~~ PJ  \n  DJ ~~ IJ  \n  DJ ~~ PJ  \n  IJ ~~ PJ\n  \n  # Means\n  ssr1 ~ ssr_int * 1\n  ssr2 ~ ssr_int * 1\n  ssr3 ~ ssr_int * 1 \n  ri1 ~ ri_int * 1\n  ri2 ~ ri_int * 1\n  ri3 ~ ri_int * 1\n  wom1 ~ wom_int * 1\n  wom2 ~ wom_int * 1\n  wom3 ~ wom_int * 1\n  sat1 ~ sat_int * 1\n  sat2 ~ sat_int * 1\n  sat3 ~ sat_int * 1\n  dj1 ~ dj_int * 1\n  dj2 ~ dj_int * 1\n  dj3 ~ dj_int * 1\n  dj4 ~ dj_int * 1\n  ij1 ~ ij_int * 1\n  ij2 ~ ij_int * 1\n  ij3 ~ ij_int * 1\n  ij4 ~ ij_int * 1\n  pj1 ~ pj_int * 1\n  pj2 ~ pj_int * 1\n  pj3 ~ pj_int * 1\n  pj4 ~ pj_int * 1\n\"\nsemPlotModel_lavaanModel(cfa_sim_model) |&gt; \n  semPaths(layout=\"tree2\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), \n           label.norm = \"OOOOO\", intAtSide = TRUE, levels = c(1,2,7,8), curvature = 2, \n           residuals = FALSE)",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#the-data",
    "href": "GenerateBaselineData.html#the-data",
    "title": "GenerateBaselineData",
    "section": "The Data",
    "text": "The Data\nLet’s use the correlation matrix data from (Fuller et al. 2016) to fit this model\n\n\nCode\n# enter as a lower triangular matrix\nlower_tri &lt;- matrix(0, ncol=7, nrow=7)\n\n# Fill the columns of the lower triangle\nlower_tri[1:1, 1] &lt;- .88\nlower_tri[1:2, 2] &lt;- c(.46, .88)\nlower_tri[1:3, 3] &lt;- c(.70, .48, .88)\nlower_tri[1:4, 4] &lt;- c(.38, .65, .34, .90)\nlower_tri[1:5, 5] &lt;- c(.64, .50, .53, .51, .89)\nlower_tri[1:6, 6] &lt;- c(.55, .47, .57, .52, .57, .89)\nlower_tri[1:7, 7] &lt;- c(.47, .44, .43, .59, .55, .55, .87)\n\n# Create the full correlation matrix\ncor_matrix &lt;- lower_tri + t(lower_tri) - diag(diag(lower_tri))\n\n# get the reliabilities\nreliabilities &lt;- diag(cor_matrix)\n\n# set the diagonal to 1\ndiag(cor_matrix) &lt;- 1\n\ncor_matrix\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,] 1.00 0.46 0.70 0.38 0.64 0.55 0.47\n[2,] 0.46 1.00 0.48 0.65 0.50 0.47 0.44\n[3,] 0.70 0.48 1.00 0.34 0.53 0.57 0.43\n[4,] 0.38 0.65 0.34 1.00 0.51 0.52 0.59\n[5,] 0.64 0.50 0.53 0.51 1.00 0.57 0.55\n[6,] 0.55 0.47 0.57 0.52 0.57 1.00 0.55\n[7,] 0.47 0.44 0.43 0.59 0.55 0.55 1.00\n\n\nThese are only the correlations. We need to use the standard deviations to get the variance-covariance matrix. At the same time let’s add the names and set the means.\n\n\nCode\nvar_names &lt;- c(\"ssr\", \"ri\", \"wom\", \"sat\", \"dj\", \"ij\", \"pj\")\n\n# Assign the names to the correlation matrix\nrownames(cor_matrix) &lt;- var_names\ncolnames(cor_matrix) &lt;- var_names\n\n# Add in the means and standard deviation:\nconstruct_means &lt;- c(SSR = 5.49, RI = 3.25, WOM = 4.03, SAT = 4.27, DJ = 3.44, IJ = 3.89, PJ = 3.93)\nconstruct_sds &lt;- c(SSR = 1.01, RI = 1.03, WOM = 0.92, SAT = 1.45, DJ = 1.38, IJ = 1.27, PJ = 1.17)\n\n#create the covariance matrix\ncov_matrix &lt;- cor2cov(cor_matrix, construct_sds) \ncov_matrix\n\n\n         ssr       ri      wom      sat       dj       ij       pj\nssr 1.020100 0.478538 0.650440 0.556510 0.892032 0.705485 0.555399\nri  0.478538 1.060900 0.454848 0.970775 0.710700 0.614807 0.530244\nwom 0.650440 0.454848 0.846400 0.453560 0.672888 0.665988 0.462852\nsat 0.556510 0.970775 0.453560 2.102500 1.020510 0.957580 1.000935\ndj  0.892032 0.710700 0.672888 1.020510 1.904400 0.998982 0.888030\nij  0.705485 0.614807 0.665988 0.957580 0.998982 1.612900 0.817245\npj  0.555399 0.530244 0.462852 1.000935 0.888030 0.817245 1.368900",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#simulating-new-data",
    "href": "GenerateBaselineData.html#simulating-new-data",
    "title": "GenerateBaselineData",
    "section": "Simulating New Data",
    "text": "Simulating New Data\nTo simulate data we need all of the right-side components of the following:\n\n\\mathbf{y_j}=\\mathbf{\\Lambda}\\mathbf\\eta_{j} + \\mathbf{\\epsilon_{j}}\n\nThe notation is as follows:\n\ny_{ij} is the response of person j\\,(j=1,...,J) on item i\\,(i=1,...,I).\n\\eta_{jk} is the kth common factor for person j.\n\\lambda_{ik} is the factor loading of item i on factor k.\n\\epsilon_{ij} is the random error term for person j on item i.\n\\mathbf{\\Psi} is the variance-covariance matrix of the common factors η_j.\n\\mathbf{\\Theta} is the variance-covariance matrix of the residuals (or unique factors) ϵ_j.\n\nIn expanded format\n\n\\underbrace{\\left[\\begin{array}{l}\ny_{1 j} \\\\\ny_{2 j} \\\\\n\\vdots \\\\\ny_{i j}\n\\end{array}\\right]}_{\\mathbf{y}_{j}}=\\underbrace{\\left[\\begin{array}{c}\n\\lambda_{11} & & 0 \\\\\n\\lambda_{21} & & 0 \\\\\n\\vdots & \\cdots & \\vdots \\\\\n\\lambda_{i1} & & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 &  & \\lambda_{ik} \\\\\n\\vdots & \\cdots & \\vdots \\\\\n0 & & \\lambda_{ik}\n\\end{array}\\right]}_{\\Lambda}\\underbrace{\\left[\\begin{array}{l}\n\\eta_{1 j} \\\\\n\\eta_{2 j} \\\\\n\\vdots \\\\\n\\eta_{i j}\n\\end{array}\\right]}_{\\mathbf{\\eta}_j}+\\underbrace{\\left[\\begin{array}{c}\n\\epsilon_{1 j} \\\\\n\\epsilon_{2 j} \\\\\n\\vdots \\\\\n\\epsilon_{i j}\n\\end{array}\\right]}_{\\mathbf{\\epsilon}_j}\n\nWhere\n\n\\mathbf{\\epsilon}_{j} \\sim N_{I}(\\mathbf{0}, \\mathbf{\\Theta})\n\n\n\\mathbf{\\eta}_{j} \\sim N_{K}(\\mathbf{\\beta}, \\mathbf{\\Psi}),\n Where \\mathbf{\\beta}_k is the mean of each construct.\nAnd\n\n\\mathbf{\\Psi}=\\mathrm{Cov}\\begin{pmatrix}\n\\eta_{1j} \\\\\n\\eta_{2j} \\\\\n\\vdots \\\\\nn_{ij}\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\psi_{11} & \\psi_{12} & \\cdots & \\psi_{1k} \\\\\n\\psi_{21} & \\psi_{22} & \\cdots & \\psi_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\psi_{i1} & \\psi_{i2} & \\cdots & \\psi_{ik}\n\\end{pmatrix}\n\n\n\\mathbf{\\Theta}=\\mathrm{Cov}\n\\begin{pmatrix}\n\\epsilon_{1 j} \\\\\n\\epsilon_{2 j} \\\\\n\\epsilon_{3 j} \\\\\n\\vdots \\\\\n\\epsilon_{i j}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta_{11} & 0 & \\cdots & 0 \\\\\n0 & \\theta_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\theta_{ii}\n\\end{pmatrix}\n\nThe covariance matrix is \\mathbf{\\Psi}, which we derived from the given correlation matrix.\n\n\nCode\npsi &lt;- cov_matrix\n\n\nFor \\mathbf{\\Theta} we need to determine the error of each item. Since we only have there reliability for each construct, we’ll need to derive each \\theta_{ii}, or the covariance for each item. We are given the reliability of each construct R_{kk}, which is an estimate of the correlation coefficient of each construct. The covariance \\epsilon_k, then, for each construct is (1 - R_{kk})^2. The linear combination of the covariance of each item x_i in construct k is \\epsilon_k = \\frac{\\sum_1^ix_{i}}{N_i}. If x_1 = x_2 ...= x_i, then \\epsilon_k = \\frac{N_ix}{N_i} = x. Therefore, each \\theta_{ii} = (1 - R_{kk})^2 for each i in each k.\nSo, for each construct we will use one minus its reliability for the covariance of its error term to construct \\mathbf{\\Theta} .\n\n\nCode\nitems_per_construct &lt;- c(3, 3, 3, 3, 4, 4, 4)\nsigma_k &lt;- (1 - reliabilities)^2 #construct error variance\ntheta &lt;- sigma_k |&gt; \n  rep(items_per_construct) |&gt; \n  diag(sum(items_per_construct), sum(items_per_construct))\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe know the average loading for each item on each construct, but we don’t know what each item loading is exactly. For now we will have each loading be 1. But we could do the following to simulate \\mathbf{\\Lambda}:\nIn measurement theory, the reliability r_{11} is defined as r_{11} = \\frac{\\sigma^2_t}{\\sigma^2_x} where t is the true factor score and x is the measured score(Nunnally and Bernstein 1994, eq. 6-9) . Therefore, \\sigma_t^2 = r_{11}\\sigma_x^2. \\sigma_x^2 is represented by \\mathbf{\\Psi}, so \\mathbf{\\Lambda} can be represented by the reliability for each construct r_{kk}\n\n\n\n\nCode\n# Create a matrix filled with zeros with rows equal to the total number of items\n# and columns equal to the number of factors\n\n#TODO change zeros to normally random loadings \nlambda &lt;- matrix(0, nrow = sum(items_per_construct), ncol = length(items_per_construct))\n\n# Fill the matrix with the 1s where each factor loads onto the corresponding items\nstart_index &lt;- 1\nfor (i in seq_along(items_per_construct)) {\n  end_index &lt;- start_index + items_per_construct[i] - 1\n  lambda[start_index:end_index, i] &lt;- reliabilities[i]\n  start_index &lt;- end_index + 1\n}\n\n\nWe can now use all of the components to generate a data set\n\n\nCode\nJ &lt;- 500\n\n# generate random factor scores for each construct on each subject\neta &lt;- MASS::mvrnorm(J, mu = construct_means, Sigma = psi)\n\n# generate error terms for each item for each subject\nepsilon &lt;- MASS::mvrnorm(J, mu = rep(0, ncol(theta)), Sigma = theta)\n\n\nY &lt;- tcrossprod(eta, lambda) + epsilon \n\nY &lt;- Y |&gt; round() # turn in to likert.  Should already be normally distributed\nY[Y &lt;= 0] &lt;- 1 #capture those that end up just below 0.5 and just above 7.5\nY[Y &gt;= 8] &lt;- 7\n\nitem_names &lt;- mapply(\\(name, num) \n                     paste0(name, seq(num)), var_names, items_per_construct, SIMPLIFY = FALSE) |&gt;\n  unlist()\n\ncfa_data  &lt;-  Y |&gt; as.data.frame() |&gt; setNames(item_names)\ncfa_data\n\n\n\n  \n\n\n\nPlot each variable to check if it is reasonable\n\n\nCode\ncfa_data |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  ggplot(aes(value)) +\n  geom_histogram(binwidth = .5) +\n  scale_x_continuous(breaks = seq(1, 7, 1), limits = c(0.5, 7.5)) +\n  facet_wrap(vars(name))\n\n\n\n\n\n\n\n\n\nSave the simulated data to allow for later use.\n\n\nCode\ncfa_data |&gt; readr::write_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#download-the-data",
    "href": "GenerateBaselineData.html#download-the-data",
    "title": "GenerateBaselineData",
    "section": "Download the Data",
    "text": "Download the Data\nClick on the button to download the data\n\n\n Download data",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#check-the-simulated-data",
    "href": "GenerateBaselineData.html#check-the-simulated-data",
    "title": "GenerateBaselineData",
    "section": "Check the Simulated Data",
    "text": "Check the Simulated Data\nLet’s see what the results are when we try to fit the data to the model\n\n\nCode\nfit&lt;- cfa(cfa_sim_model, cfa_data, meanstructure = TRUE)\nsummary(fit)\n\n\nlavaan 0.6-18 ended normally after 186 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        93\n  Number of equality constraints                    17\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               228.552\n  Degrees of freedom                               248\n  P-value (Chi-square)                           0.807\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  SSR =~                                              \n    ssr1              1.000                           \n    ssr2              0.996    0.018   54.588    0.000\n    ssr3              1.001    0.019   52.101    0.000\n  RI =~                                               \n    ri1               1.000                           \n    ri2               1.026    0.020   52.242    0.000\n    ri3               1.012    0.019   51.926    0.000\n  WOM =~                                              \n    wom1              1.000                           \n    wom2              0.990    0.022   45.692    0.000\n    wom3              0.978    0.021   45.999    0.000\n  SAT =~                                              \n    sat1              1.000                           \n    sat2              0.986    0.012   84.325    0.000\n    sat3              0.987    0.011   91.751    0.000\n  DJ =~                                               \n    dj1               1.000                           \n    dj2               1.016    0.012   81.315    0.000\n    dj3               0.995    0.013   76.804    0.000\n    dj4               1.012    0.013   79.722    0.000\n  IJ =~                                               \n    ij1               1.000                           \n    ij2               1.003    0.014   71.530    0.000\n    ij3               1.015    0.014   71.102    0.000\n    ij4               1.032    0.014   72.309    0.000\n  PJ =~                                               \n    pj1               1.000                           \n    pj2               0.987    0.016   60.292    0.000\n    pj3               0.989    0.016   62.188    0.000\n    pj4               1.019    0.016   65.079    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  SSR ~~                                              \n    RI                0.376    0.042    8.900    0.000\n    WOM               0.540    0.044   12.361    0.000\n    SAT               0.429    0.059    7.253    0.000\n    DJ                0.712    0.060   11.771    0.000\n    IJ                0.588    0.057   10.277    0.000\n    PJ                0.439    0.049    8.909    0.000\n  RI ~~                                               \n    WOM               0.329    0.038    8.722    0.000\n    SAT               0.760    0.065   11.743    0.000\n    DJ                0.553    0.056    9.956    0.000\n    IJ                0.501    0.054    9.260    0.000\n    PJ                0.422    0.048    8.811    0.000\n  WOM ~~                                              \n    SAT               0.368    0.053    6.973    0.000\n    DJ                0.514    0.051   10.029    0.000\n    IJ                0.548    0.052   10.594    0.000\n    PJ                0.386    0.044    8.775    0.000\n  SAT ~~                                              \n    DJ                0.751    0.079    9.472    0.000\n    IJ                0.878    0.082   10.757    0.000\n    PJ                0.854    0.075   11.461    0.000\n  DJ ~~                                               \n    IJ                0.828    0.075   11.075    0.000\n    PJ                0.708    0.066   10.718    0.000\n  IJ ~~                                               \n    PJ                0.727    0.066   10.988    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ssr1    (ssr_)    4.833    0.042  116.017    0.000\n   .ssr2    (ssr_)    4.833    0.042  116.017    0.000\n   .ssr3    (ssr_)    4.833    0.042  116.017    0.000\n   .ri1     (r_nt)    2.875    0.041   69.899    0.000\n   .ri2     (r_nt)    2.875    0.041   69.899    0.000\n   .ri3     (r_nt)    2.875    0.041   69.899    0.000\n   .wom1    (wm_n)    3.515    0.037   95.306    0.000\n   .wom2    (wm_n)    3.515    0.037   95.306    0.000\n   .wom3    (wm_n)    3.515    0.037   95.306    0.000\n   .sat1    (st_n)    3.887    0.059   66.390    0.000\n   .sat2    (st_n)    3.887    0.059   66.390    0.000\n   .sat3    (st_n)    3.887    0.059   66.390    0.000\n   .dj1     (dj_n)    3.045    0.054   56.730    0.000\n   .dj2     (dj_n)    3.045    0.054   56.730    0.000\n   .dj3     (dj_n)    3.045    0.054   56.730    0.000\n   .dj4     (dj_n)    3.045    0.054   56.730    0.000\n   .ij1     (ij_n)    3.467    0.053   64.868    0.000\n   .ij2     (ij_n)    3.467    0.053   64.868    0.000\n   .ij3     (ij_n)    3.467    0.053   64.868    0.000\n   .ij4     (ij_n)    3.467    0.053   64.868    0.000\n   .pj1     (pj_n)    3.400    0.047   72.003    0.000\n   .pj2     (pj_n)    3.400    0.047   72.003    0.000\n   .pj3     (pj_n)    3.400    0.047   72.003    0.000\n   .pj4     (pj_n)    3.400    0.047   72.003    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    SSR               0.857    0.059   14.513    0.000\n    RI                0.812    0.056   14.551    0.000\n    WOM               0.679    0.048   14.222    0.000\n    SAT               1.748    0.113   15.437    0.000\n    DJ                1.435    0.094   15.256    0.000\n    IJ                1.408    0.094   15.034    0.000\n    PJ                1.119    0.075   15.007    0.000\n   .ssr1              0.076    0.007   10.904    0.000\n   .ssr2              0.057    0.006    9.287    0.000\n   .ssr3              0.073    0.007   10.620    0.000\n   .ri1               0.069    0.007   10.196    0.000\n   .ri2               0.072    0.007   10.133    0.000\n   .ri3               0.072    0.007   10.295    0.000\n   .wom1              0.075    0.007   10.427    0.000\n   .wom2              0.072    0.007   10.368    0.000\n   .wom3              0.068    0.007   10.187    0.000\n   .sat1              0.042    0.005    8.117    0.000\n   .sat2              0.075    0.006   11.729    0.000\n   .sat3              0.057    0.006   10.128    0.000\n   .dj1               0.052    0.005   11.315    0.000\n   .dj2               0.056    0.005   11.465    0.000\n   .dj3               0.066    0.005   12.337    0.000\n   .dj4               0.060    0.005   11.800    0.000\n   .ij1               0.073    0.006   12.295    0.000\n   .ij2               0.062    0.005   11.581    0.000\n   .ij3               0.065    0.006   11.695    0.000\n   .ij4               0.062    0.005   11.363    0.000\n   .pj1               0.060    0.006   10.861    0.000\n   .pj2               0.087    0.007   12.532    0.000\n   .pj3               0.078    0.006   12.141    0.000\n   .pj4               0.070    0.006   11.435    0.000",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#footnotes",
    "href": "GenerateBaselineData.html#footnotes",
    "title": "GenerateBaselineData",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn (Maxham and Netemeyer 2003), There’s a “Purchase Intent”, but not a “Return Intent”.↩︎",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "CMVSimulationWithLavaan.html",
    "href": "CMVSimulationWithLavaan.html",
    "title": "CMV Simulation using Lavaan",
    "section": "",
    "text": "This document will go through simulating CMV and IEP with the JBR data using Lavaan’s simulateData() function."
  },
  {
    "objectID": "CMVSimulationWithLavaan.html#cmv-simulation",
    "href": "CMVSimulationWithLavaan.html#cmv-simulation",
    "title": "CMV Simulation using Lavaan",
    "section": "",
    "text": "This document will go through simulating CMV and IEP with the JBR data using Lavaan’s simulateData() function."
  },
  {
    "objectID": "CMVSimulationWithLavaan.html#initialization",
    "href": "CMVSimulationWithLavaan.html#initialization",
    "title": "CMV Simulation using Lavaan",
    "section": "Initialization",
    "text": "Initialization\nWe will need the following projects:\n\ntidyverse: Data wrangling\nlavaan: SEM functionality\nsemPlot: Plotting SEM diagrams\n\nWe’ll also set the random seed to an arbitrary fixed value to allow for reproduceability\n\n\nCode\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nset.seed(1337)"
  },
  {
    "objectID": "CMVSimulationWithLavaan.html#model",
    "href": "CMVSimulationWithLavaan.html#model",
    "title": "CMV Simulation using Lavaan",
    "section": "Model",
    "text": "Model\nOur model has three latent variables that are each measured with several items:\n\nMobile phone addiction: phone_add\n\nMissed Work: mw\nConcentration: conc\nPain: pain\nRequire Phone: reqp\nImpatient: imp\nThinking: think\nNever Give Up: ngu\nConstant Checking: check\nExtended Usage: use\nPerceived Extended Usage: puse\n\nPerceived Stress: stress\n\nUpset: upset\nUnable to Control: nocont\nNervous: nerv\nIrritating Hassles: hass\nCoping: cope\nConfident: conf\nGoing Your Way: way\nNo Cope: nocope\nControl: cont\nOn Top: ontop\nAngered: angr\nAnxious: anx\nControl Time: time\nPiling: pile\n\nPerceived Learning: plearn\n\nOrganize: org\nGuide: guid\nUse Knowledge: usek\nAttitude: att\nCritique: crit\nSelf-Reliant: selfr\nSophisticated: soph\n\n\nWe’ll input the model in lavaan formula syntax and have it create an SEM diagram for us\n\n\nCode\ncfa_model &lt;- '\n  # measurement model\n  phone_add =~ mw + conc + pain + reqp + imp + think + ngu + check + use + puse\n  stress =~ upset + nocont + nerv + hass + cop + conf + way + nocope + cont + ontop + angr + anx + time + pile\n  plearn =~ org + guid + usek + att + crit + selfr + soph\n  \n  # covariances\n  phone_add ~~ stress\n  phone_add ~~ plearn\n  stress ~~ plearn\n'\n\nsemPlotModel_lavaanModel(cfa_model) |&gt; \n  semPaths(layout=\"tree\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), label.norm = \"OOOOO\", residuals = FALSE)"
  },
  {
    "objectID": "CMVSimulationWithLavaan.html#simulation",
    "href": "CMVSimulationWithLavaan.html#simulation",
    "title": "CMV Simulation using Lavaan",
    "section": "Simulation",
    "text": "Simulation\nNow we’ll simulate data for this model. In this case all item weights are simulated as being equal\n\n\n\n\n\n\nNote\n\n\n\nRight now everything is coded in the same direction. Will need to fix loading at -1 for reverse coded items\n\n\n\n\nCode\ncfa_sim &lt;- simulateData(cfa_model, model.type = \"cfa\", sample.nobs = 500)\n#change to likert scores\ncfa_sim &lt;- cfa_sim |&gt; mutate(across(everything(), \\(x) findInterval(x, vec=c(-Inf,-2.5,-1, 1,2.5,Inf))))\ncfa_sim\n\n\n\n  \n\n\n\nCheck the fit of the simulation\n\n\nCode\nfit &lt;- cfa(cfa_model, cfa_sim, ordered = TRUE)\nsummary(fit)\n\n\nlavaan 0.6-18 ended normally after 43 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                       158\n\n  Number of observations                           500\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               347.483     465.802\n  Degrees of freedom                               431         431\n  P-value (Chi-square)                           0.999       0.120\n  Scaling correction factor                                  1.329\n  Shift parameter                                          204.263\n    simple second-order correction                                \n\nParameter Estimates:\n\n  Parameterization                               Delta\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  phone_add =~                                        \n    mw                1.000                           \n    conc              1.012    0.045   22.424    0.000\n    pain              0.980    0.046   21.419    0.000\n    reqp              0.977    0.046   21.256    0.000\n    imp               0.999    0.045   22.125    0.000\n    think             0.957    0.047   20.349    0.000\n    ngu               0.949    0.050   19.164    0.000\n    check             0.922    0.047   19.455    0.000\n    use               0.989    0.046   21.612    0.000\n    puse              1.026    0.048   21.494    0.000\n  stress =~                                           \n    upset             1.000                           \n    nocont            1.000    0.043   23.132    0.000\n    nerv              1.055    0.046   22.838    0.000\n    hass              1.048    0.042   25.067    0.000\n    cop               0.991    0.048   20.530    0.000\n    conf              0.989    0.047   20.915    0.000\n    way               1.070    0.046   23.247    0.000\n    nocope            1.020    0.044   23.027    0.000\n    cont              0.960    0.045   21.191    0.000\n    ontop             1.005    0.044   23.094    0.000\n    angr              0.992    0.046   21.478    0.000\n    anx               1.007    0.044   22.784    0.000\n    time              0.905    0.049   18.556    0.000\n    pile              0.996    0.045   22.191    0.000\n  plearn =~                                           \n    org               1.000                           \n    guid              1.055    0.054   19.475    0.000\n    usek              0.972    0.053   18.495    0.000\n    att               1.014    0.057   17.890    0.000\n    crit              1.015    0.057   17.867    0.000\n    selfr             1.018    0.055   18.444    0.000\n    soph              1.083    0.054   20.086    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  phone_add ~~                                        \n    stress            0.005    0.025    0.196    0.845\n    plearn           -0.013    0.025   -0.533    0.594\n  stress ~~                                           \n    plearn            0.031    0.024    1.284    0.199\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    mw|t1            -1.825    0.108  -16.973    0.000\n    mw|t2            -0.668    0.061  -10.971    0.000\n    mw|t3             0.739    0.062   11.909    0.000\n    mw|t4             1.607    0.092   17.416    0.000\n    conc|t1          -1.706    0.099  -17.297    0.000\n    conc|t2          -0.687    0.061  -11.228    0.000\n    conc|t3           0.745    0.062   11.993    0.000\n    conc|t4           1.774    0.104   17.134    0.000\n    pain|t1          -1.977    0.121  -16.300    0.000\n    pain|t2          -0.656    0.061  -10.800    0.000\n    pain|t3           0.745    0.062   11.993    0.000\n    pain|t4           1.799    0.105   17.059    0.000\n    reqp|t1          -1.728    0.100  -17.252    0.000\n    reqp|t2          -0.706    0.062  -11.484    0.000\n    reqp|t3           0.700    0.061   11.399    0.000\n    reqp|t4           1.774    0.104   17.134    0.000\n    imp|t1           -1.728    0.100  -17.252    0.000\n    imp|t2           -0.650    0.061  -10.714    0.000\n    imp|t3            0.706    0.062   11.484    0.000\n    imp|t4            1.774    0.104   17.134    0.000\n    think|t1         -1.825    0.108  -16.973    0.000\n    think|t2         -0.693    0.061  -11.314    0.000\n    think|t3          0.706    0.062   11.484    0.000\n    think|t4          1.825    0.108   16.973    0.000\n    ngu|t1           -1.706    0.099  -17.297    0.000\n    ngu|t2           -0.687    0.061  -11.228    0.000\n    ngu|t3            0.732    0.062   11.824    0.000\n    ngu|t4            1.852    0.110   16.873    0.000\n    check|t1         -1.645    0.095  -17.388    0.000\n    check|t2         -0.662    0.061  -10.886    0.000\n    check|t3          0.793    0.063   12.580    0.000\n    check|t4          1.852    0.110   16.873    0.000\n    use|t1           -1.852    0.110  -16.873    0.000\n    use|t2           -0.650    0.061  -10.714    0.000\n    use|t3            0.726    0.062   11.739    0.000\n    use|t4            1.799    0.105   17.059    0.000\n    puse|t1          -1.665    0.096  -17.364    0.000\n    puse|t2          -0.687    0.061  -11.228    0.000\n    puse|t3           0.772    0.063   12.330    0.000\n    puse|t4           1.943    0.118   16.474    0.000\n    upset|t1         -1.706    0.099  -17.297    0.000\n    upset|t2         -0.713    0.062  -11.570    0.000\n    upset|t3          0.674    0.061   11.057    0.000\n    upset|t4          1.751    0.102   17.198    0.000\n    nocont|t1        -1.728    0.100  -17.252    0.000\n    nocont|t2        -0.637    0.060  -10.541    0.000\n    nocont|t3         0.687    0.061   11.228    0.000\n    nocont|t4         1.943    0.118   16.474    0.000\n    nerv|t1          -1.685    0.097  -17.334    0.000\n    nerv|t2          -0.739    0.062  -11.909    0.000\n    nerv|t3           0.719    0.062   11.655    0.000\n    nerv|t4           1.852    0.110   16.873    0.000\n    hass|t1          -1.751    0.102  -17.198    0.000\n    hass|t2          -0.765    0.063  -12.246    0.000\n    hass|t3           0.662    0.061   10.886    0.000\n    hass|t4           1.751    0.102   17.198    0.000\n    cop|t1           -1.626    0.093  -17.405    0.000\n    cop|t2           -0.662    0.061  -10.886    0.000\n    cop|t3            0.706    0.062   11.484    0.000\n    cop|t4            1.665    0.096   17.364    0.000\n    conf|t1          -1.555    0.089  -17.421    0.000\n    conf|t2          -0.668    0.061  -10.971    0.000\n    conf|t3           0.693    0.061   11.314    0.000\n    conf|t4           1.825    0.108   16.973    0.000\n    way|t1           -1.751    0.102  -17.198    0.000\n    way|t2           -0.713    0.062  -11.570    0.000\n    way|t3            0.745    0.062   11.993    0.000\n    way|t4            1.685    0.097   17.334    0.000\n    nocope|t1        -1.911    0.115  -16.626    0.000\n    nocope|t2        -0.779    0.063  -12.413    0.000\n    nocope|t3         0.759    0.062   12.162    0.000\n    nocope|t4         1.665    0.096   17.364    0.000\n    cont|t1          -1.706    0.099  -17.297    0.000\n    cont|t2          -0.719    0.062  -11.655    0.000\n    cont|t3           0.687    0.061   11.228    0.000\n    cont|t4           1.665    0.096   17.364    0.000\n    ontop|t1         -1.607    0.092  -17.416    0.000\n    ontop|t2         -0.619    0.060  -10.282    0.000\n    ontop|t3          0.668    0.061   10.971    0.000\n    ontop|t4          1.685    0.097   17.334    0.000\n    angr|t1          -1.706    0.099  -17.297    0.000\n    angr|t2          -0.687    0.061  -11.228    0.000\n    angr|t3           0.765    0.063   12.246    0.000\n    angr|t4           1.825    0.108   16.973    0.000\n    anx|t1           -1.589    0.091  -17.423    0.000\n    anx|t2           -0.625    0.060  -10.368    0.000\n    anx|t3            0.732    0.062   11.824    0.000\n    anx|t4            1.706    0.099   17.297    0.000\n    time|t1          -1.665    0.096  -17.364    0.000\n    time|t2          -0.681    0.061  -11.143    0.000\n    time|t3           0.625    0.060   10.368    0.000\n    time|t4           1.665    0.096   17.364    0.000\n    pile|t1          -1.825    0.108  -16.973    0.000\n    pile|t2          -0.732    0.062  -11.824    0.000\n    pile|t3           0.726    0.062   11.739    0.000\n    pile|t4           1.706    0.099   17.297    0.000\n    org|t1           -1.751    0.102  -17.198    0.000\n    org|t2           -0.700    0.061  -11.399    0.000\n    org|t3            0.662    0.061   10.886    0.000\n    org|t4            1.728    0.100   17.252    0.000\n    guid|t1          -1.706    0.099  -17.297    0.000\n    guid|t2          -0.719    0.062  -11.655    0.000\n    guid|t3           0.719    0.062   11.655    0.000\n    guid|t4           1.774    0.104   17.134    0.000\n    usek|t1          -1.728    0.100  -17.252    0.000\n    usek|t2          -0.700    0.061  -11.399    0.000\n    usek|t3           0.739    0.062   11.909    0.000\n    usek|t4           1.881    0.112   16.758    0.000\n    att|t1           -1.943    0.118  -16.474    0.000\n    att|t2           -0.779    0.063  -12.413    0.000\n    att|t3            0.631    0.060   10.455    0.000\n    att|t4            1.607    0.092   17.416    0.000\n    crit|t1          -1.685    0.097  -17.334    0.000\n    crit|t2          -0.687    0.061  -11.228    0.000\n    crit|t3           0.706    0.062   11.484    0.000\n    crit|t4           1.685    0.097   17.334    0.000\n    selfr|t1         -1.881    0.112  -16.758    0.000\n    selfr|t2         -0.813    0.063  -12.829    0.000\n    selfr|t3          0.662    0.061   10.886    0.000\n    selfr|t4          1.589    0.091   17.423    0.000\n    soph|t1          -1.645    0.095  -17.388    0.000\n    soph|t2          -0.713    0.062  -11.570    0.000\n    soph|t3           0.619    0.060   10.282    0.000\n    soph|t4           1.728    0.100   17.252    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .mw                0.467                           \n   .conc              0.453                           \n   .pain              0.488                           \n   .reqp              0.491                           \n   .imp               0.467                           \n   .think             0.512                           \n   .ngu               0.519                           \n   .check             0.546                           \n   .use               0.478                           \n   .puse              0.439                           \n   .upset             0.483                           \n   .nocont            0.483                           \n   .nerv              0.424                           \n   .hass              0.432                           \n   .cop               0.493                           \n   .conf              0.494                           \n   .way               0.408                           \n   .nocope            0.462                           \n   .cont              0.523                           \n   .ontop             0.478                           \n   .angr              0.491                           \n   .anx               0.476                           \n   .time              0.576                           \n   .pile              0.488                           \n   .org               0.510                           \n   .guid              0.454                           \n   .usek              0.537                           \n   .att               0.496                           \n   .crit              0.495                           \n   .selfr             0.493                           \n   .soph              0.426                           \n    phone_add         0.533    0.038   14.018    0.000\n    stress            0.517    0.037   14.144    0.000\n    plearn            0.490    0.041   11.905    0.000"
  },
  {
    "objectID": "CMVSimulationWithLavaan.html#means-and-standard-deviations",
    "href": "CMVSimulationWithLavaan.html#means-and-standard-deviations",
    "title": "CMV Simulation using Lavaan",
    "section": "Means and Standard Deviations",
    "text": "Means and Standard Deviations\nWe can add means and standard deviations to the latent variables by adding intercepts and variances\n\n\nCode\ncfa_sim_model_with_means &lt;- '\n  # measurement model\n  phone_add =~ mw + conc + pain + reqp + imp + think + ngu + check + use + puse\n  stress =~ upset + nocont + nerv + hass + cop + conf + way + nocope + cont + ontop + angr + anx + time + pile\n  plearn =~ org + guid + usek + att + crit + selfr + soph\n  \n  # covariances\n  phone_add ~~ stress\n  phone_add ~~ plearn\n  stress ~~ plearn\n  \n  # intercepts (means)\n  # set the means of each of the phone addiction measurements to 2\n  mw + conc + pain + reqp + imp + think + ngu + check + use + puse ~ 2*1\n  # set the means of each of the negative stress measurments to 4 but the postitive ones to 2\n  upset + nocont + nerv + hass + way + nocope + cont + ontop + angr + anx + time + pile ~ 4*1\n  cop + conf ~ 2*1\n  # set the means of the plearn measurments to 4\n  org + guid + usek + att + crit + selfr + soph ~ 3*1\n  \n  # variances\n  phone_add ~~ 1 * phone_add\n  stress ~~ 16 * stress\n  plearn ~~ 1 * plearn\n'\n\nsemPlotModel_lavaanModel(cfa_sim_model_with_means) |&gt; \n  semPaths(layout=\"tree2\", rotation=2, levels = c(5,6,11,12), nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), label.norm = \"OOOOO\", residuals = TRUE, intAtSide = TRUE)\n\n\n\n\n\n\n\n\n\n\nSimulate and fit with means and standard deviations\n\n\nCode\ncfa_sim_with_means &lt;- simulateData(cfa_sim_model_with_means, model.type = \"cfa\", sample.nobs = 500)\n#change to likert scores\ncfa_sim_with_means &lt;- cfa_sim_with_means |&gt; mutate(across(everything(), \\(x) findInterval(x, vec=c(-Inf,-2.5,-1, 1,2.5,Inf))))\ncfa_sim_with_means\n\n\n\n  \n\n\n\nCheck the fit of the simulation. First we need to remove the fixed means and variances\n\n\nCode\ncfa_model_with_means &lt;- '\n  # measurement model\n  phone_add =~ mw + conc + pain + reqp + imp + think + ngu + check + use + puse\n  stress =~ upset + nocont + nerv + hass + cop + conf + way + nocope + cont + ontop + angr + anx + time + pile\n  plearn =~ org + guid + usek + att + crit + selfr + soph\n  \n  # covariances\n  phone_add ~~ stress\n  phone_add ~~ plearn\n  stress ~~ plearn\n  \n  # intercepts (means)\n  mw + conc + pain + reqp + imp + think + ngu + check + use + puse ~ 1\n  upset + nocont + nerv + hass + way + nocope + cont + ontop + angr + anx + time + pile ~ 1\n  cop + conf ~ 1\n  org + guid + usek + att + crit + selfr + soph ~ 1\n  \n  # variances\n  phone_add ~~ phone_add\n  stress ~~ stress\n  plearn ~~ plearn\n'\n\nfit &lt;- cfa(cfa_model_with_means, cfa_sim_with_means) # ordered = TRUE does not identify\nsummary(fit)\n\n\nlavaan 0.6-18 ended normally after 69 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               640.062\n  Degrees of freedom                               431\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  phone_add =~                                        \n    mw                1.000                           \n    conc              1.002    0.080   12.527    0.000\n    pain              0.964    0.079   12.141    0.000\n    reqp              1.033    0.079   13.011    0.000\n    imp               1.051    0.083   12.677    0.000\n    think             1.059    0.083   12.804    0.000\n    ngu               0.997    0.080   12.459    0.000\n    check             1.007    0.079   12.714    0.000\n    use               1.039    0.080   12.925    0.000\n    puse              0.990    0.081   12.284    0.000\n  stress =~                                           \n    upset             1.000                           \n    nocont            1.018    0.024   43.046    0.000\n    nerv              1.033    0.024   43.821    0.000\n    hass              1.029    0.023   44.137    0.000\n    cop               1.172    0.032   36.673    0.000\n    conf              1.167    0.032   37.016    0.000\n    way               1.037    0.023   44.805    0.000\n    nocope            1.009    0.023   43.279    0.000\n    cont              1.007    0.025   41.096    0.000\n    ontop             1.058    0.023   45.554    0.000\n    angr              1.005    0.023   43.505    0.000\n    anx               1.048    0.025   42.481    0.000\n    time              1.024    0.024   43.187    0.000\n    pile              1.038    0.022   46.385    0.000\n  plearn =~                                           \n    org               1.000                           \n    guid              1.231    0.117   10.529    0.000\n    usek              1.061    0.107    9.894    0.000\n    att               1.125    0.108   10.389    0.000\n    crit              1.135    0.109   10.407    0.000\n    selfr             1.173    0.113   10.394    0.000\n    soph              1.161    0.113   10.294    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  phone_add ~~                                        \n    stress            0.041    0.028    1.499    0.134\n    plearn           -0.007    0.010   -0.746    0.456\n  stress ~~                                           \n    plearn           -0.008    0.019   -0.439    0.661\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .mw                4.116    0.036  114.462    0.000\n   .conc              4.104    0.036  114.964    0.000\n   .pain              4.118    0.036  115.456    0.000\n   .reqp              4.142    0.035  117.831    0.000\n   .imp               4.080    0.037  110.505    0.000\n   .think             4.050    0.037  110.186    0.000\n   .ngu               4.082    0.036  114.160    0.000\n   .check             4.060    0.035  115.259    0.000\n   .use               4.086    0.036  114.692    0.000\n   .puse              4.088    0.036  113.184    0.000\n   .upset             4.286    0.052   81.695    0.000\n   .nocont            4.272    0.054   79.770    0.000\n   .nerv              4.256    0.054   78.694    0.000\n   .hass              4.252    0.054   78.999    0.000\n   .way               4.258    0.054   78.813    0.000\n   .nocope            4.270    0.053   80.549    0.000\n   .cont              4.266    0.054   79.624    0.000\n   .ontop             4.240    0.055   77.145    0.000\n   .angr              4.254    0.053   80.697    0.000\n   .anx               4.238    0.055   76.629    0.000\n   .time              4.268    0.054   79.305    0.000\n   .pile              4.226    0.054   78.667    0.000\n   .cop               3.700    0.064   57.505    0.000\n   .conf              3.706    0.064   58.026    0.000\n   .org               4.578    0.027  167.857    0.000\n   .guid              4.544    0.030  151.120    0.000\n   .usek              4.524    0.028  158.874    0.000\n   .att               4.564    0.028  162.606    0.000\n   .crit              4.558    0.028  161.425    0.000\n   .selfr             4.552    0.029  155.712    0.000\n   .soph              4.550    0.029  154.884    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    phone_add         0.270    0.035    7.704    0.000\n    stress            1.217    0.087   14.064    0.000\n    plearn            0.124    0.019    6.376    0.000\n   .mw                0.376    0.026   14.317    0.000\n   .conc              0.366    0.026   14.268    0.000\n   .pain              0.385    0.027   14.453    0.000\n   .reqp              0.329    0.024   13.986    0.000\n   .imp               0.383    0.027   14.188    0.000\n   .think             0.372    0.026   14.115    0.000\n   .ngu               0.371    0.026   14.303    0.000\n   .check             0.347    0.024   14.167    0.000\n   .use               0.343    0.024   14.041    0.000\n   .puse              0.387    0.027   14.388    0.000\n   .upset             0.159    0.011   14.635    0.000\n   .nocont            0.172    0.012   14.685    0.000\n   .nerv              0.165    0.011   14.601    0.000\n   .hass              0.159    0.011   14.565    0.000\n   .cop               0.397    0.026   15.165    0.000\n   .conf              0.382    0.025   15.146    0.000\n   .way               0.151    0.010   14.484    0.000\n   .nocope            0.165    0.011   14.660    0.000\n   .cont              0.200    0.013   14.865    0.000\n   .ontop             0.147    0.010   14.384    0.000\n   .angr              0.161    0.011   14.636    0.000\n   .anx               0.192    0.013   14.741    0.000\n   .time              0.172    0.012   14.670    0.000\n   .pile              0.130    0.009   14.262    0.000\n   .org               0.248    0.018   13.980    0.000\n   .guid              0.265    0.020   13.202    0.000\n   .usek              0.266    0.019   13.888    0.000\n   .att               0.238    0.018   13.380    0.000\n   .crit              0.240    0.018   13.359    0.000\n   .selfr             0.257    0.019   13.375    0.000\n   .soph              0.265    0.020   13.492    0.000"
  },
  {
    "objectID": "SimpleCMVSimulation.html",
    "href": "SimpleCMVSimulation.html",
    "title": "Simple CVM Simulation",
    "section": "",
    "text": "In this document, we’ll do a simple CMV Simulation to show that our technique works. We’ll then use the same technique on our real data"
  },
  {
    "objectID": "SimpleCMVSimulation.html#simple-cmv-simulation",
    "href": "SimpleCMVSimulation.html#simple-cmv-simulation",
    "title": "Simple CVM Simulation",
    "section": "",
    "text": "In this document, we’ll do a simple CMV Simulation to show that our technique works. We’ll then use the same technique on our real data"
  },
  {
    "objectID": "SimpleCMVSimulation.html#initialization",
    "href": "SimpleCMVSimulation.html#initialization",
    "title": "Simple CVM Simulation",
    "section": "Initialization",
    "text": "Initialization\nWe will need the following projects:\n\ntidyverse: Data wrangling\nlavaan: SEM functionality\nsimstandard: Does lavaan simulation properly (lavaan’s SimulateData() has problems) and provides some handy functions for working with SEM data in data frames\nsemPlot: Plotting SEM diagrams\n\nWe’ll also set the random seed to an arbitrary fixed value to allow for reproduceability\n\n\nCode\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(simstandard)\nlibrary(semPlot)\n\nset.seed(1337)"
  },
  {
    "objectID": "SimpleCMVSimulation.html#model",
    "href": "SimpleCMVSimulation.html#model",
    "title": "Simple CVM Simulation",
    "section": "Model",
    "text": "Model\nOur model has three latent variables that are each measured with two or three items. Additionally, each item is measured using a 5-likert scale.\n\nX\n\nx1\nx2\nx3\n\nY\n\ny1\ny2\ny3\n\nZ\n\nz1\nz2\nz3\n\n\nWe’ll input the model in lavaan formula syntax and have it create an SEM diagram for us\n\n\nCode\ncfa_model &lt;- '\n  # measurement model\n  X =~ x1 + x2 + x3\n  Y =~ y1 + y2 + y3\n  Z =~ z1 + z2 + z3\n  \n  # covariances\n  X ~~ Y\n  Y ~~ Z\n  X ~~ Z\n'\n\nsemPlotModel_lavaanModel(cfa_model) |&gt; \n  semPaths(layout=\"tree\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), label.norm = \"OOOOO\", residuals = FALSE)"
  },
  {
    "objectID": "SimpleCMVSimulation.html#simulation",
    "href": "SimpleCMVSimulation.html#simulation",
    "title": "Simple CVM Simulation",
    "section": "Simulation",
    "text": "Simulation\nTo simulate, we need to set the parameter values. We’ll do this for each kind of parameter. Before we do that, though, we need to convert the model to a lavaan parameter table.\n\n\nCode\ncfa_model_pars &lt;- lavaanify(cfa_model, std.lv = TRUE)\n\n\n\nparameters\nWe’ll create a value for each parameter\n\n\nCode\nloadings &lt;- 0.6\ncovariances &lt;- 0.1\ncfa_model_pars &lt;- cfa_model_pars |&gt; \n  mutate(ustart = ifelse(op == \"=~\", loadings, ustart)) |&gt;\n  rowwise() |&gt; mutate(ustart = ifelse(op == \"~~\" && lhs != rhs, covariances, ustart)) |&gt; ungroup() |&gt; as.data.frame()\n\n\n\n\nCode\ncfa_sim &lt;- sim_standardized(cfa_model_pars, latent = FALSE, errors = FALSE, factor_scores = FALSE, composites = FALSE, matrices = FALSE)\n\n\nWarning: lavaan-&gt;lavParTable():  \n   input already looks like a parameter table\n\n\nWarning in sim_standardized_matrices(m, ...): Because the following relationship was not set, it is assumed to be 0:\nX =~ x1\n\n\nWarning: lavaan-&gt;lavaanify():  \n   input already looks like a parameter table\n\n\nCode\n#cfa_sim &lt;- simulateData(cfa_model_pars, model.type = \"cfa\")\n#change to likert scores\ncfa_sim &lt;- cfa_sim |&gt; mutate(across(everything(), \\(x) findInterval(x, vec=c(-Inf,-1,-0.5, 0.5,1,Inf))))\ncfa_sim\n\n\n\n  \n\n\n\nCheck the fit of the simulation\n\n\nCode\nfit &lt;- cfa(cfa_model, cfa_sim, ordered = TRUE, parameterization = \"theta\", std.lv = TRUE)\nsummary(fit, fit.measures = TRUE)\n\n\nlavaan 0.6-18 ended normally after 39 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        48\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                14.408      18.929\n  Degrees of freedom                                24          24\n  P-value (Chi-square)                           0.937       0.756\n  Scaling correction factor                                  0.868\n  Shift parameter                                            2.332\n    simple second-order correction                                \n\nModel Test Baseline Model:\n\n  Test statistic                              1341.312    1126.235\n  Degrees of freedom                                36          36\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.197\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.011       1.007\n                                                                  \n  Robust Comparative Fit Index (CFI)                         1.000\n  Robust Tucker-Lewis Index (TLI)                            1.014\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.000\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.006       0.019\n  P-value H_0: RMSEA &lt;= 0.050                    1.000       1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.000\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.015\n  P-value H_0: Robust RMSEA &lt;= 0.050                         1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.020       0.020\n\nParameter Estimates:\n\n  Parameterization                               Theta\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  X =~                                                \n    x1                0.772    0.082    9.397    0.000\n    x2                0.745    0.077    9.682    0.000\n    x3                0.766    0.083    9.245    0.000\n  Y =~                                                \n    y1                0.772    0.085    9.082    0.000\n    y2                0.760    0.087    8.739    0.000\n    y3                0.673    0.072    9.369    0.000\n  Z =~                                                \n    z1                0.701    0.069   10.207    0.000\n    z2                0.871    0.087    9.967    0.000\n    z3                0.820    0.081   10.162    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  X ~~                                                \n    Y                 0.074    0.053    1.398    0.162\n  Y ~~                                                \n    Z                 0.008    0.051    0.153    0.879\n  X ~~                                                \n    Z                 0.158    0.051    3.133    0.002\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    x1|t1            -1.337    0.076  -17.544    0.000\n    x1|t2            -0.652    0.057  -11.474    0.000\n    x1|t3             0.673    0.058   11.560    0.000\n    x1|t4             1.267    0.075   16.976    0.000\n    x2|t1            -1.146    0.067  -17.056    0.000\n    x2|t2            -0.545    0.054  -10.175    0.000\n    x2|t3             0.686    0.057   12.053    0.000\n    x2|t4             1.303    0.073   17.873    0.000\n    x3|t1            -1.295    0.075  -17.163    0.000\n    x3|t2            -0.632    0.056  -11.197    0.000\n    x3|t3             0.675    0.058   11.613    0.000\n    x3|t4             1.289    0.077   16.824    0.000\n    y1|t1            -1.298    0.076  -16.996    0.000\n    y1|t2            -0.641    0.057  -11.265    0.000\n    y1|t3             0.605    0.057   10.641    0.000\n    y1|t4             1.205    0.072   16.654    0.000\n    y2|t1            -1.265    0.076  -16.564    0.000\n    y2|t2            -0.677    0.057  -11.782    0.000\n    y2|t3             0.532    0.056    9.570    0.000\n    y2|t4             1.126    0.071   15.791    0.000\n    y3|t1            -1.239    0.067  -18.603    0.000\n    y3|t2            -0.629    0.053  -11.894    0.000\n    y3|t3             0.587    0.053   11.141    0.000\n    y3|t4             1.199    0.066   18.024    0.000\n    z1|t1            -1.240    0.067  -18.538    0.000\n    z1|t2            -0.592    0.053  -11.156    0.000\n    z1|t3             0.616    0.053   11.635    0.000\n    z1|t4             1.137    0.064   17.803    0.000\n    z2|t1            -1.386    0.082  -16.996    0.000\n    z2|t2            -0.673    0.061  -11.073    0.000\n    z2|t3             0.661    0.059   11.240    0.000\n    z2|t4             1.292    0.076   16.955    0.000\n    z3|t1            -1.302    0.076  -17.111    0.000\n    z3|t2            -0.612    0.057  -10.707    0.000\n    z3|t3             0.616    0.057   10.854    0.000\n    z3|t4             1.329    0.075   17.780    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                1.000                           \n   .x2                1.000                           \n   .x3                1.000                           \n   .y1                1.000                           \n   .y2                1.000                           \n   .y3                1.000                           \n   .z1                1.000                           \n   .z2                1.000                           \n   .z3                1.000                           \n    X                 1.000                           \n    Y                 1.000                           \n    Z                 1.000                           \n\nScales y*:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    x1                0.791                           \n    x2                0.802                           \n    x3                0.794                           \n    y1                0.792                           \n    y2                0.796                           \n    y3                0.830                           \n    z1                0.819                           \n    z2                0.754                           \n    z3                0.773"
  },
  {
    "objectID": "SimpleCMVSimulation.html#model-with-cmv",
    "href": "SimpleCMVSimulation.html#model-with-cmv",
    "title": "Simple CVM Simulation",
    "section": "Model with CMV",
    "text": "Model with CMV\nLet’s add the CMV\n\n\nCode\ncmv_model &lt;- '\n  # measurement model\n  X =~ 0.6*x1 + 0.6*x2 + 0.6*x3\n  Y =~ 0.6*y1 + 0.6*y2 + 0.6*y3\n  Z =~ 0.6*z1 + 0.6*z2 + 0.6*z3\n  CMV =~ 0.5*x1 + 0.5*x2 + 0.5*x3 + 0.5*y1 + 0.5*y2 + 0.5*z1 + 0.5*z2 + 0.5*z3\n\n  # covariances\n  X ~~ 0.1*Y\n  Y ~~ 0.1*Z\n  X ~~ 0.1*Z\n  X ~~ 0*CMV\n  Y ~~ 0*CMV\n  Z ~~ 0*CMV\n'\n\nsemPlotModel_lavaanModel(cmv_model) |&gt; \n  semPaths(layout=\"tree\", rotation=2, nCharNodes=5, sizeMan2 = 2.5, mar = c(1,4,1,3), label.norm = \"OOOOO\", residuals = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# lmodel &lt;- lavaanify(cmv_model, std.lv = TRUE)\n# loadings &lt;- 0.6\n# covariances &lt;- 0.1\n# cmv &lt;- 0\n# lmodel &lt;- lmodel |&gt; rowwise() |&gt;\n#   mutate(ustart = ifelse(op == \"=~\" && lhs != \"CMV\", loadings, ustart)) |&gt;\n#   mutate(ustart = ifelse(op == \"=~\" && lhs == \"CMV\", cmv, ustart)) |&gt;\n#   mutate(ustart = ifelse(op == \"~~\" && lhs != rhs, covariances, ustart)) |&gt; ungroup() |&gt; as.data.frame()\n\n\nSimulate and fit this one\n\n\nCode\ncmv_sim &lt;- sim_standardized(cmv_model, latent = FALSE, errors = FALSE, factor_scores = FALSE, composites = FALSE, matrices = FALSE)\n#change to likert scores\ncmv_sim &lt;- cmv_sim |&gt; mutate(across(everything(), \\(x) findInterval(x, vec=c(-Inf,-1,-0.5, 0.5,1,Inf))))\ncmv_sim\n\n\n\n  \n\n\n\nCheck the fit of the simulation\n\n\nCode\ncmv_fit &lt;- cfa(cfa_model, cmv_sim, ordered = TRUE, parameterization = \"theta\", std.lv = TRUE)\n#cmv_fit &lt;- cfa(cfa_model, cmv_sim, std.lv = TRUE)\nsummary(cmv_fit, fit.measures = TRUE)\n\n\nlavaan 0.6-18 ended normally after 54 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        48\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                43.084      71.015\n  Degrees of freedom                                24          24\n  P-value (Chi-square)                           0.010       0.000\n  Scaling correction factor                                  0.639\n  Shift parameter                                            3.539\n    simple second-order correction                                \n\nModel Test Baseline Model:\n\n  Test statistic                              6289.301    4080.706\n  Degrees of freedom                                36          36\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.546\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997       0.988\n  Tucker-Lewis Index (TLI)                       0.995       0.983\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.992\n  Robust Tucker-Lewis Index (TLI)                            0.988\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.028       0.044\n  90 Percent confidence interval - lower         0.014       0.033\n  90 Percent confidence interval - upper         0.042       0.056\n  P-value H_0: RMSEA &lt;= 0.050                    0.997       0.770\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.032\n  90 Percent confidence interval - lower                     0.013\n  90 Percent confidence interval - upper                     0.049\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.960\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033       0.033\n\nParameter Estimates:\n\n  Parameterization                               Theta\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  X =~                                                \n    x1                1.300    0.092   14.130    0.000\n    x2                1.308    0.102   12.792    0.000\n    x3                1.203    0.087   13.749    0.000\n  Y =~                                                \n    y1                1.291    0.133    9.683    0.000\n    y2                1.425    0.162    8.776    0.000\n    y3                0.419    0.045    9.278    0.000\n  Z =~                                                \n    z1                1.180    0.090   13.159    0.000\n    z2                1.159    0.085   13.620    0.000\n    z3                1.173    0.085   13.869    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  X ~~                                                \n    Y                 0.417    0.036   11.628    0.000\n  Y ~~                                                \n    Z                 0.414    0.037   11.329    0.000\n  X ~~                                                \n    Z                 0.473    0.035   13.629    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    x1|t1            -1.658    0.097  -17.022    0.000\n    x1|t2            -0.836    0.074  -11.267    0.000\n    x1|t3             0.827    0.074   11.108    0.000\n    x1|t4             1.658    0.097   17.170    0.000\n    x2|t1            -1.658    0.105  -15.763    0.000\n    x2|t2            -0.873    0.077  -11.310    0.000\n    x2|t3             0.715    0.074    9.730    0.000\n    x2|t4             1.452    0.095   15.216    0.000\n    x3|t1            -1.601    0.094  -17.122    0.000\n    x3|t2            -0.866    0.072  -11.965    0.000\n    x3|t3             0.776    0.070   11.035    0.000\n    x3|t4             1.518    0.090   16.842    0.000\n    y1|t1            -1.714    0.124  -13.834    0.000\n    y1|t2            -0.861    0.084  -10.287    0.000\n    y1|t3             0.828    0.085    9.726    0.000\n    y1|t4             1.644    0.125   13.136    0.000\n    y2|t1            -1.746    0.148  -11.832    0.000\n    y2|t2            -0.964    0.098   -9.800    0.000\n    y2|t3             0.824    0.094    8.751    0.000\n    y2|t4             1.634    0.140   11.657    0.000\n    y3|t1            -1.092    0.053  -20.621    0.000\n    y3|t2            -0.597    0.046  -13.043    0.000\n    y3|t3             0.513    0.045   11.359    0.000\n    y3|t4             1.074    0.053   20.385    0.000\n    z1|t1            -1.721    0.099  -17.454    0.000\n    z1|t2            -0.883    0.073  -12.057    0.000\n    z1|t3             0.749    0.070   10.713    0.000\n    z1|t4             1.526    0.092   16.588    0.000\n    z2|t1            -1.600    0.092  -17.368    0.000\n    z2|t2            -0.838    0.070  -11.996    0.000\n    z2|t3             0.686    0.068   10.134    0.000\n    z2|t4             1.473    0.088   16.763    0.000\n    z3|t1            -1.558    0.089  -17.485    0.000\n    z3|t2            -0.817    0.070  -11.639    0.000\n    z3|t3             0.756    0.069   10.992    0.000\n    z3|t4             1.495    0.086   17.290    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                1.000                           \n   .x2                1.000                           \n   .x3                1.000                           \n   .y1                1.000                           \n   .y2                1.000                           \n   .y3                1.000                           \n   .z1                1.000                           \n   .z2                1.000                           \n   .z3                1.000                           \n    X                 1.000                           \n    Y                 1.000                           \n    Z                 1.000                           \n\nScales y*:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    x1                0.610                           \n    x2                0.607                           \n    x3                0.639                           \n    y1                0.612                           \n    y2                0.574                           \n    y3                0.922                           \n    z1                0.647                           \n    z2                0.653                           \n    z3                0.649                           \n\n\nCompare the two models\n\n\nCode\nanova(fit, cmv_fit)\n\n\nWarning: lavaan-&gt;lavTestLRT():  \n   some models have the same degrees of freedom"
  }
]